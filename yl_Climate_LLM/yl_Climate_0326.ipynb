{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd5b7a91-129a-49ba-a888-ccef4b85cfa9",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0b6acc-e332-4fe3-b950-f6f1cfc965e6",
   "metadata": {},
   "source": [
    "**Paper idea**:\n",
    "\n",
    "Our LLM model to do the Q&A task:\n",
    "\n",
    "Question -> Function_call(LLM1) -> input -> prediction model -> output -> LLM2 -> Answer\n",
    "\n",
    "**Ex**: \n",
    "\n",
    "Question: \"Will coastal New York see the impact of Greenland ice sheet melting by 2100?\"\n",
    "\n",
    "Function_call(LLM1): 'gpt-3.5-turbo'\n",
    "\n",
    "input: 2100\n",
    "\n",
    "prediction model: \"GMSL_prediction_SEM.csv\"\n",
    "\n",
    "output: 'The prediction of global mean sea level in 2100 is 686.690760499435. The 95% of confidence interval is from 604.018417351968 to 769.363103646902.'\n",
    "\n",
    "LLM2: 'gpt-3.5-turbo'\n",
    "\n",
    "Answer: 'Based on the provided sea level prediction for 2100 and the 95% confidence interval, coastal New York is likely to see the impact of Greenland ice sheet melting by 2100. The predicted global mean sea level rise of 686.690760499435 by 2100 falls within the 95% confidence interval of 604.018417351968 to 769.363103646902. This indicates a high probability of sea level rise that could result from the melting of the Greenland ice sheet, which would impact coastal areas like New York.'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac22977",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Previous result:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a516e51-795a-4598-843f-36d8fd7f3221",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "(3.19)Papers:\n",
    "\n",
    "ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements? (28 Nov 2023) https://arxiv.org/abs/2311.17107\n",
    "\n",
    "Large Language Models: A Survey (20 Feb 2024) https://arxiv.org/abs/2402.06196\n",
    "\n",
    "An Interactive Agent Foundation Model (8 Feb 2024) https://arxiv.org/abs/2402.05929\n",
    "\n",
    "HuggingGPT: Solving AI Tasks with ChatGPT and its Friends in Hugging Face (3 Dec 2023) https://arxiv.org/abs/2303.17580\n",
    "\n",
    "Video as the New Language for Real-World Decision Making (27 Feb 2024) https://arxiv.org/abs/2402.17139\n",
    "\n",
    "\n",
    "(3.12)Paper idea:(Needs to discuss)\n",
    "\n",
    "1. Task(Experiment):\n",
    "- Climate related\n",
    "-  Ex. Questions&Answers, fact-checking or anything else\n",
    "\n",
    "2. Methods: \n",
    "- Retrieval (RAG) ChatGPT, Llama2, Gemma, Claude\n",
    "- Function call ChatGPT, Claude\n",
    "\n",
    "3. Agreement\n",
    "- Create a new Large language based model and apply it to climate area.\n",
    "- solve hallucination and outdated information\n",
    "\n",
    "My idea:\n",
    "\n",
    "1. Task(Experiment): Sea level related Questions&Answers\n",
    "2. Methods: LLM + Sea level prediction function (+ other prediction functions)\n",
    "\n",
    "(3.5)Tool related papers:\n",
    "\n",
    "Gorilla: Large Language Model Connected with Massive APIs (24 May 2023) https://arxiv.org/abs/2305.15334\n",
    "\n",
    "ART: Automatic multi-step reasoning and tool-use for large language models (16 Mar 2023) https://arxiv.org/abs/2303.09014\n",
    "\n",
    "https://github.com/google/BIG-bench/blob/main/bigbench/benchmark_tasks/keywords_to_tasks.md#summary-table\n",
    "\n",
    "ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs (3 Oct 2023) https://arxiv.org/abs/2307.16789\n",
    "\n",
    "ToolkenGPT: Augmenting Frozen Language Models with Massive Tools via Tool Embeddings (15 Jan 2024) https://arxiv.org/abs/2305.11554\n",
    "\n",
    "Feature Table:\n",
    "\n",
    "| Feature              | ChatGPT-4 | ChatClimate | Climinator | Our |\n",
    "|----------------------|-----------|-------------|------------|-----|\n",
    "| Climate Specific     |           |       ✅      |     ✅       |  ✅   |\n",
    "| External information |           |        ✅     |    ✅        |   ✅  |\n",
    "| Tool use             |           |             |            | ✅    |\n",
    "| Single step          |     ✅      |    ✅         |            |     |\n",
    "| Multi step           |           |             |         ✅   |  ✅   |\n",
    "\n",
    "\n",
    "(2.27)Fact-checking paper\n",
    "\n",
    "Paper idea:\n",
    "\n",
    "Answer the question: What is the weather in New York right now?\n",
    "\n",
    "https://platform.openai.com/docs/guides/function-calling (2023.6.23)\n",
    "\n",
    "(2.20)Paper Presentation: Automated Fact-Checking of Climate Change Claims with Large Language Models (23 Jan 2024) https://arxiv.org/abs/2401.12566\n",
    "\n",
    "Slides https://github.com/yl1127/Academic-projects/blob/main/yl_Climate_LLM/yl_CLIMINATOR_final_0220.pdf\n",
    "\n",
    "Visualization Discussion:\n",
    "1. other split method(By default: Split by character ) https://python.langchain.com/docs/modules/data_connection/document_transformers/#evaluate-text-splitters\n",
    "3. Create collection(By default: chroma) https://python.langchain.com/docs/modules/data_connection/vectorstores/\n",
    "       https://python.langchain.com/docs/integrations/vectorstores\n",
    "4. Visualization space https://umap-learn.readthedocs.io/en/latest/index.html\n",
    "\n",
    "Paper idea : solve issues (hallucination, outdated information and prediction)\n",
    "1. reply some exist data (RAG same as the chatclimate)\n",
    "2. do the prediction (Agent let the LLM be able to use tool like our prediction model Autoregressive Distributed Lag (ARDL))\n",
    "3. Answer the question like \"Will coastal New York see the impact of Greenland ice sheet melting by 2100?\"\n",
    "\n",
    "(2.12)New mehtod: RAG details and visualize.\n",
    "\n",
    "(2.6)Papers: \n",
    "\n",
    "ClimateBert: A Pretrained Language Model for Climate-Related Text (22 Oct 2021) https://arxiv.org/abs/2110.12010\n",
    "\n",
    "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks (22 May 2020) https://arxiv.org/abs/2005.11401\n",
    "\n",
    "Query Expansion by Prompting Large Language Models (5 May 2023) https://arxiv.org/abs/2305.03653\n",
    "\n",
    "PLLaMa: An Open-source Large Language Model for Plant Science (3 Jan 2024) https://arxiv.org/abs/2401.01600\n",
    "\n",
    "GenCast: Diffusion-based ensemble forecasting for medium-range weather (25 Dec 2023) https://arxiv.org/abs/2312.15796\n",
    "\n",
    "\n",
    "Google docs:\n",
    "\n",
    "Q1.5: \"Have emissions reductions fallen for some countries?\"\n",
    "\n",
    "Q2.2: \"Is it possible the Arctic will become ice free at some points before 2050?\"\n",
    "\n",
    "https://docs.google.com/document/d/1HGypipUiT0u045w-9PcUgCWJAXOX4fqpFQNKhjiG6ys/edit?usp=sharing\n",
    "\n",
    "All question: https://docs.google.com/document/d/1AEu75SflCQhwFmI4dBdG58RIPq7hzhkN6ip9h19dv0o/edit?usp=sharing\n",
    "\n",
    "\n",
    "\n",
    "(1.31)New model: [Bard(Original)](https://bard.google.com/chat) For 13 questions from paper, [13 answers](https://g.co/bard/share/117570e457af) For 7 new questions, [7 answers](https://g.co/bard/share/644ebd6a4f50) API [Join Waitlist](https://cloud.google.com/ai/earlyaccess/join?hl=en)\n",
    "\n",
    "(1.22) View the all [answers](yl_Appendix.ipynb).\n",
    "\n",
    "(1.16) 7 questions:\n",
    "\n",
    "1. What is the best estimate of the equilibrium climate sensitivity?\n",
    "2. Is it possible the Arctic will become ice free at some points before 2050?\n",
    "3. How likely will the world see runaway ice loss from the Antarctic  ice sheets by 2100?\n",
    "4. To what extent has climate change impacted ice sheets in both polar regions?\n",
    "5. Will coastal New York see the impact of Greenland ice sheet melting by 2100?\n",
    "6. What will be the likely changes to weather and climate extremes in coastal New York around 2050 if the world take no action to reduce greenhouse gas emissions?\n",
    "7. What are the primary evidence and how robust is it that human activities caused the observed global warming since the pre-industrial period?\n",
    "\n",
    "The 13 original questions are from [chatClimate: Grounding Conversational AI in Climate Science](https://arxiv.org/abs/2304.05510)\n",
    "\n",
    "\n",
    "\n",
    "The Climate report is from [IPCC_AR6_WGIII_FullReport](https://sdgs.un.org/sites/default/files/2023-01/IPCC_AR6_WGIII_FullReport.pdf)\n",
    "\n",
    "(1.2)The Llama2(Original), ChatGPT-4(Original), Llama2+(Climate report) and Llama2 hybird code parts are available [here](https://github.com/yl1127/Academic-projects/blob/main/yl_Climate_LLM/yl_Climate_0102.ipynb).\n",
    "\n",
    "\n",
    "(2023.12.18)The Llama2(Original) and Llama2+(Climate report) code parts are available [here](https://github.com/yl1127/Academic-projects/blob/main/yl_Climate_LLM/yl_LlamaCloud_Climate.ipynb).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9aa786ad-539d-4f13-8c79-0594cf1be007",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAs the sun set over the horizon, the sky transformed into a canvas of vibrant hues. The once bright blue sky gradually shifted to a warm orange, pink, and purple, casting a soft glow over the landscape. The stars began to twinkle one by one, like diamonds scattered across the velvet expanse.\\n\\nThe village lay still and quiet, shrouded in an eerie silence. The residents had long since retired to their homes, seeking refuge from the heat of the day. The only sound that could be heard was the distant chirping of crickets and the occasional hooting of owls.\\n\\nAs the night deepened, a sense of mystery and enchantment began to settle over the village. Strange noises could be heard in the distance, like whispers carried on the wind. The villagers had grown accustomed to these eerie sounds, but they still sent shivers down their spines.\\n\\nIn one of the small cottages nestled at the edge of the forest, a young girl named Lily lay fast asleep. She had been exhausted from a long day of helping her mother with chores and tending to their garden. But as she slept, strange dreams began to haunt her.\\n\\nShe saw herself walking through the forest, surrounded by towering trees that seemed to stretch up to the sky. The moon cast eerie shadows on the ground, making it look like there were monsters lurking in every corner. Lily tried to run but felt as though she was running in place, unable to escape the darkness that closed in around her.\\n\\nSuddenly, a figure appeared before her. It was tall and imposing, with eyes that glowed like embers in the night. Lily felt a shiver run down her spine as the figure spoke in a low, raspy voice. \"I have been waiting for you, little one,\" it said. \"You have been chosen to fulfill an ancient prophecy.\"\\n\\nLily tried to speak but found that she couldn\\'t move her lips. She felt as though she was trapped in a nightmare from which she could not awaken. The figure began to whisper strange incantations, and the air around Lily began to shimmer and glow.\\n\\nShe felt herself being lifted off the ground, and before she knew it, she found herself standing in a clearing deep within the forest. The moon hung low in the sky, casting an eerie light over the landscape. In the center of the clearing stood an ancient tree, its trunk twisted and gnarled with age.\\n\\nLily felt drawn to the tree as though it were calling out to her. She reached out a hand to touch it, and suddenly, she felt a surge of energy coursing through her body. The tree began to glow with an otherworldly light, and Lily knew that she had been chosen for a great purpose.\\n\\nAs the night wore on, Lily found herself wandering deeper into the forest, led by some unseen force. She encountered strange creatures and encountered mysterious symbols etched into the bark of the trees. The air was thick with magic and wonder, and Lily felt as though she had entered a dream world that was both exhilarating and terrifying.\\n\\nAs the first light of dawn began to creep over the horizon, Lily found herself back in her own bed, still feeling the echoes of her nighttime adventure. She knew that her life would never be the same again, and that she had been chosen for a great purpose. But as she drifted off to sleep, she couldn\\'t shake the feeling that the night was only just beginning, and that there were still many secrets waiting to be uncovered in the magical world of the forest.\\n\\nThe village stirred back to life as the sun rose higher in the sky. The villagers emerged from their homes, blinking away the sleepiness of the night. They exchanged quiet greetings and set about their daily routines, unaware of the magic that had transpired under the cover of darkness.\\n\\nBut Lily knew. She felt it deep within her bones, a sense of purpose and destiny that she could not ignore. She knew that she would return to the forest again and again, seeking out the secrets that lay hidden there. And as she walked through the village, she carried with her the whispered magic of the night, knowing that it would stay with her always.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama2:13b\")\n",
    "# llm = Ollama(model=\"gemma:7b\")\n",
    "\n",
    "llm.invoke(\"Give me a 1000 words night story \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ea03a-cc69-45b0-80d3-664e48ca6831",
   "metadata": {},
   "source": [
    "\n",
    "* How to reproduce fact-checking paper\n",
    "* How to run an agent app to answer current weather condition?\n",
    "* How to run ChatGPT in the cloud hosted on OpenAI\n",
    "* How to use LangChain to load a recent PDF doc - This is the well known RAG (Retrieval Augmented Generation) method to let LLM such as Llama2 be able to answer questions about the data not publicly available when Llama2 was trained, or about your own data. RAG is one way to prevent LLM's hallucination\n",
    "* How to visualize the Embedding space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dde626",
   "metadata": {},
   "source": [
    "Let's start by installing the necessary packages:\n",
    "- sentence-transformers for text embeddings\n",
    "- chromadb gives us database capabilities \n",
    "- langchain provides necessary RAG tools for this demo\n",
    "\n",
    "And setting up the Replicate token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd8508-c8b8-4cd2-be26-01b97e664ff9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## How to set up the helper function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8870c1",
   "metadata": {},
   "source": [
    "Next we call the ChatGPT model from OpenAI. In this example we will use the ChatGPT-3.5-turbo chat model. You can find more ChatGPT models by searching for them on the [OpenAI API keys](https://platform.openai.com/api-keys).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f928862f-3003-4f86-ba2b-b13a11b1df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API\n",
    "from getpass import getpass\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "new_api_key = getpass()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = new_api_key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "966e9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install OpenAI\n",
    "# !pip install typing-extensions --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccd13b79-8382-400b-b536-ad225d0d689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI()\n",
    "\n",
    "def get_completion_from_messages(messages, \n",
    "                                 model=\"gpt-3.5-turbo\", \n",
    "                                 temperature=0, \n",
    "                                 max_tokens=500):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature, # this is the degree of randomness of the model's output\n",
    "        max_tokens=max_tokens, # the maximum number of tokens the model can ouptut \n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd207c80",
   "metadata": {},
   "source": [
    "With the model set up, you are now ready to ask some questions. Here is an example of the simplest way to ask the model some general questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963dfcde-c82e-4fd1-a6ea-f3fc12cceb5f",
   "metadata": {},
   "source": [
    "### Prompt Example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68d8a896-0fbf-415e-840f-bbdcfbd196f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once upon a time, there was a golden retriever named Max. Max lived in a cozy house with his loving family who took great care of him. Max was a very happy dog who loved nothing more than playing fetch in the backyard, going for long walks in the park, and cuddling up with his family on the couch.\n",
      "\n",
      "Every morning, Max would wake up with a wagging tail and a big smile on his face. He would greet his family with kisses and tail wags, eager to start the day. Max's family made sure he had everything he needed - a comfortable bed, plenty of toys to play with, and nutritious food to eat.\n",
      "\n",
      "One day, Max's family decided to take him on a special trip to the beach. Max had never been to the beach before, and he was filled with excitement as they drove closer to the shore. When they arrived, Max couldn't contain his joy. He ran around in the sand, chasing seagulls and splashing in the waves. Max's family laughed and cheered him on as he raced up and down the beach, his fur glistening in the sun.\n",
      "\n",
      "As the day went on, Max's family set up a picnic on the sand. They shared sandwiches and snacks, while Max lounged nearby, soaking in the sun. Max felt so grateful for his loving family and the beautiful day they were sharing together.\n",
      "\n",
      "As the sun began to set, Max curled up next to his family, exhausted but content. He closed his eyes, dreaming of all the adventures they would have together in the future. Max knew that no matter what, he would always be a happy dog, surrounded by the love of his family.\n",
      "\n",
      "And so, the happy dog drifted off to sleep, his tail thumping softly against the sand, grateful for the wonderful life he had been blessed with.\n"
     ]
    }
   ],
   "source": [
    "messages =  [{'role':'user', 'content':'write me a story about a happy dog'}]\n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdeb5174-711a-4809-8c5a-8471799bdcff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Once there was a joyful dog named Buddy who wagged his tail nonstop as he bounded through the fields, spreading happiness wherever he went.\n"
     ]
    }
   ],
   "source": [
    "# length\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'All your responses must be \\\n",
    "one sentence long.'},    \n",
    "{'role':'user',\n",
    " 'content':'write me a story about a happy dog'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08ad0ada-aad7-4b44-89a6-1579aaf4285d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a cozy town by the sea,  \n",
      "Lived a dog named Max so full of glee.  \n",
      "With fur as golden as the sun,  \n",
      "Max found joy in every run.  \n",
      "\n",
      "From dawn till dusk, he'd play and leap,  \n",
      "Chasing squirrels and counting sheep.  \n",
      "His tail would wag, his eyes would shine,  \n",
      "In Max's world, life was divine.  \n",
      "\n",
      "He'd greet each friend with bark and cheer,  \n",
      "Bringing smiles far and near.  \n",
      "With a heart so pure and love so true,  \n",
      "Max knew just what to say and do.  \n",
      "\n",
      "In fields of flowers, by the shore,  \n",
      "Max would frolic, bark, and explore.  \n",
      "With every bound and every bark,  \n",
      "He left a trail of joy in his spark.  \n",
      "\n",
      "And so dear Max, so happy and free,  \n",
      "Spread love and joy for all to see.  \n",
      "A tale of a dog so pure and bright,  \n",
      "Who made the world a bit more light.\n"
     ]
    }
   ],
   "source": [
    "# poem style\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'You are an assistant who responds in the style of poem.'},    \n",
    " {'role':'user',\n",
    " 'content':'write me a story about a happy dog'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10c122d-2bcc-4742-9bce-31613d02c5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a meadow gleaming with the sun, a golden retriever frolicked, pure joy unfurled.\n"
     ]
    }
   ],
   "source": [
    "# Combine\n",
    "messages =  [  \n",
    "{'role':'system',\n",
    " 'content':'You are an assistant who responds in the style of poem. \\\n",
    "     All your responses must be one sentence long.'},    \n",
    " {'role':'user',\n",
    " 'content':'write me a story about a happy dog'},  \n",
    "] \n",
    "response = get_completion_from_messages(messages, temperature =1)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3492ce3e-d507-48f9-b3cb-7269ffa527a0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Retrieval (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08366ebd-7ed5-4a88-8b11-0d5c79ed1127",
   "metadata": {},
   "source": [
    "### How to load an external PDF and do embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5303d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95254686-d140-4187-b9a4-fc39195e52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Group III contribution to the\n",
      "Sixth Assessment Report of the \n",
      "Intergovernmental Panel on Climate ChangeWGIIIMitigation of Climate ChangeClimate Change 2022\n"
     ]
    }
   ],
   "source": [
    "# from langchain.document_loaders import PyPDFLoader\n",
    "from pypdf import PdfReader\n",
    "loader = PdfReader(\"IPCC_AR6_WGIII_FullReport.pdf\")\n",
    "# docs = loader.load()\n",
    "pdf_texts = [p.extract_text().strip() for p in loader.pages]\n",
    "pdf_texts = [text for text in pdf_texts if text]\n",
    "print(pdf_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678c2b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030\n",
      "Front cover photograph: Matt Bridgestock, Director and Architect at John Gilbert Architects\n",
      "All International Energy Agency (IEA) Data, IEA Further Data and Derived Data has been  \n",
      "sourced from https://www.iea.org/data-and-statistics.\n",
      "© 2022 Intergovernmental Panel on Climate Change.\n",
      "Electronic copies of this Summary for Policymakers are available from the IPCC website www.ipcc.ch\n",
      "ISBN 978-92-9169-160-9\n"
     ]
    }
   ],
   "source": [
    "# check docs length and content\n",
    "# print(len(docs), docs[0].page_content[0:300])\n",
    "# print(docs[100])\n",
    "print(len(pdf_texts))\n",
    "print(pdf_texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8268e",
   "metadata": {},
   "source": [
    "We need to store our documents. There are more than 30 vector stores (DBs) supported by LangChain. \n",
    "For this example we will use [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) which is light-weight and in memory so it's easy to get started with.\n",
    "For other vector stores especially if you need to store a large amount of data - see https://python.langchain.com/docs/integrations/vectorstores\n",
    "\n",
    "We will also import the HuggingFaceEmbeddings and RecursiveCharacterTextSplitter to assist in storing the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0787375-c30c-468d-83ee-95e55a99a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "853a3002-8987-41d2-83f5-7b2406aaa6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approaches falling outside the scope of individual sectors. \n",
      "As in the AR5, there is a chapter on recent trends and drivers, with \n",
      "the scope expanded to cover historic emissions and recent policy \n",
      "developments. Following the pattern established in the WG III AR5 \n",
      "report, and the Special Report on Global Warming of 1.5°C, this \n",
      "report assesses published emission scenarios with a 21st century \n",
      "perspective. Modelled emission scenarios are categorised according \n",
      "to climate outcomes, allowing a handshake with the WG I assessment. \n",
      "To meet the goal of linking top-down and bottom-up insights, the \n",
      "report includes an additional pathways chapter that provides a mid-\n",
      "century perspective, focussing on national and regional scales and the \n",
      "alignment between development pathways and mitigation actions.\n",
      "As in the AR5, this report addresses mitigation enablers such as \n",
      "international cooperation, finance and investment, and policies and\n",
      "\n",
      "Total chunks: 13302\n"
     ]
    }
   ],
   "source": [
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "\n",
    "print(character_split_texts[10])\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4a17c",
   "metadata": {},
   "source": [
    "To store the documents, we will need to split them into chunks using [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) and create vector representations of these chunks using Embedding on them before storing them into our vector database. \n",
    "\n",
    "In general, you should use larger chuck sizes for highly structured text such as code and smaller size for less structured text. You may need to experiment with different chunk sizes and overlap values to find out the best numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47cb6410-ac1d-4f01-8b90-5a4fce754798",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0008131744107231498, 0.005816039629280567, 0.07490231841802597, 0.06837399303913116, 0.08766962587833405, -0.001249835011549294, -0.06404281407594681, -0.041942864656448364, -0.0731612890958786, 0.018257392570376396, -0.08234506100416183, -0.05028412118554115, -0.036275602877140045, 0.05759482458233833, -0.02672104351222515, 0.02087916061282158, -0.11043477058410645, -0.04460994526743889, -0.06738454848527908, -0.09205488860607147, 0.025163717567920685, 0.053215108811855316, 0.03425822779536247, -0.01038744580000639, 0.01888451538980007, -0.05462276563048363, -0.02559971623122692, 0.02336598001420498, -0.0675044134259224, 0.08059892803430557, 0.03271729499101639, 0.08507393300533295, -0.0669960305094719, -0.01698952354490757, 0.02977534383535385, 0.06567184627056122, 0.03229949250817299, 0.019549256190657616, 0.034711915999650955, 0.004731979686766863, 0.03487369045615196, -0.08469007909297943, 0.005258500110358, -0.053371354937553406, -0.03895176947116852, 0.02310674823820591, 0.029902227222919464, -0.02959704026579857, -0.09657879918813705, -0.03529023751616478, 0.04533004388213158, 0.009732253849506378, -0.028350288048386574, -0.03756144642829895, 0.016690393909811974, -0.08259609341621399, 0.024609869346022606, -0.03386127948760986, 0.030592424795031548, -0.024605881422758102, 0.04021024703979492, -0.0997571274638176, -0.05041475594043732, -0.032708194106817245, 0.013197526335716248, 0.02956053987145424, 0.020827777683734894, 0.05168427899479866, -0.0803096666932106, 0.02298329770565033, -0.015998173505067825, -0.05991421267390251, -0.037617914378643036, -0.10007443279027939, -0.04562161862850189, -0.02886272594332695, 0.04107731208205223, 0.06851903349161148, 0.136552631855011, -0.06198299303650856, 0.07946737855672836, 0.05629274621605873, 0.045702315866947174, -0.014305220916867256, -0.00916312262415886, 0.01594240963459015, 0.02562369965016842, -0.055027373135089874, 0.015188025310635567, 0.010028718970716, -0.08051285892724991, -0.10780766606330872, 0.11219824850559235, 0.06828777492046356, 0.009419424459338188, 0.049908142536878586, 0.0525018572807312, -0.07206861674785614, -0.03957438841462135, -0.02523641102015972, 0.05405263230204582, 0.03120727650821209, -0.031230570748448372, -0.014256633818149567, -0.045183029025793076, -0.04755415394902229, 0.033494073897600174, -0.018141167238354683, -0.0028857928700745106, -0.08269155770540237, -0.02051335945725441, -0.0038964347913861275, 0.030972257256507874, -0.10416172444820404, -0.005552905611693859, -0.030283937230706215, 0.05470116809010506, -0.03452908620238304, 0.0067615513689816, -0.057414062321186066, -0.05361006036400795, 0.029483536258339882, -0.008551438339054585, 0.03311788663268089, 0.027616040781140327, 0.0009091763058677316, -0.022346092388033867, -3.148198179701452e-34, -0.02378738299012184, 0.04083821922540665, -0.024610105901956558, 0.055880848318338394, -0.027471639215946198, 0.010531463660299778, -0.03506100922822952, -0.021030638366937637, -0.049931466579437256, 0.021935278549790382, 0.037974707782268524, 0.03312178701162338, -0.015208257362246513, 0.044570934027433395, 0.09951746463775635, -0.03827711194753647, -0.06261847913265228, 0.07422149926424026, -0.004499478731304407, 0.04742663353681564, -0.01949765533208847, -0.04350997507572174, 0.051540788263082504, 0.004180451389402151, 0.05637999251484871, 0.0553116500377655, 0.08011173456907272, -0.03806518763303757, -0.03197339177131653, 0.0249946266412735, 0.03321279585361481, 0.06589790433645248, -0.04928840324282646, -0.02228543721139431, -0.03054862469434738, 0.004628081806004047, -0.05025926232337952, 0.00042081219726242125, -0.051273614168167114, 0.03598637878894806, -0.018657058477401733, 0.040235936641693115, -0.06646499037742615, -0.02033206820487976, 0.08865194022655487, 0.053962621837854385, 0.05879061669111252, -0.0017052236944437027, -0.0428924597799778, -0.012654351070523262, -0.06561136245727539, 0.043590303510427475, -0.011275216937065125, -0.1174880862236023, 0.007202413398772478, -0.015101253055036068, 0.012693336233496666, -0.062399499118328094, -0.022247884422540665, -0.046464934945106506, -0.007934140041470528, -0.04324405640363693, -0.04633884131908417, 0.014951729215681553, -0.009135439060628414, 0.07956354320049286, 0.004399281460791826, 0.013315612450242043, 0.007010743487626314, 0.0439438596367836, 0.013059155084192753, -0.02631806768476963, 0.05496981367468834, 0.05290668085217476, 0.03567369282245636, 0.03608066588640213, 0.02690526656806469, 0.05146677792072296, 0.02813364937901497, 0.034605465829372406, -0.09991007298231125, 0.009389725513756275, 0.02039894461631775, -0.10734459012746811, 0.014839597046375275, -0.06565370410680771, 0.03533736616373062, 0.02838641032576561, 0.043854400515556335, -0.0007286188192665577, -0.12002497911453247, -0.006314447149634361, 0.026596952229738235, 0.043239153921604156, -0.021579479798674583, -1.3903657182739782e-33, 0.08855462074279785, 0.034720778465270996, 0.005496637895703316, -0.0422530472278595, -0.10618706792593002, -0.04063175246119499, -0.01252723764628172, -0.08295296877622604, 0.04795144125819206, 0.01763845980167389, -0.0011188788339495659, 0.016154751181602478, 0.06258921325206757, 0.05396806076169014, 0.005201827734708786, -0.08168455958366394, 0.0049345484003424644, -0.06701385229825974, 0.041503194719552994, -0.018674198538064957, 0.07067793607711792, -0.12410937249660492, -0.022031636908650398, 0.05769239738583565, -0.04759272560477257, 0.06218743696808815, 0.04384215548634529, -0.08019555360078812, 0.0660632997751236, -0.0697801485657692, -0.07979913055896759, 0.05919814854860306, -0.04232003167271614, 0.012405341491103172, -0.07576002925634384, 0.009405984543263912, 0.072294220328331, -0.06410615146160126, -0.060173869132995605, -0.04750730097293854, 0.01781412586569786, -0.029355116188526154, -0.020721858367323875, -0.009146858006715775, -0.037026967853307724, -0.00035890823346562684, 0.01554032415151596, 0.09361839294433594, -0.004884979221969843, -0.0696963369846344, 0.06942164897918701, 0.03710677847266197, -0.07186876982450485, 0.004690141882747412, -0.027291173115372658, 0.02135000191628933, 0.10465742647647858, 0.01540087815374136, -0.02435462549328804, 0.032575637102127075, 0.003950105048716068, 0.08484233915805817, 0.06916497647762299, 0.06575360894203186, -0.014882882125675678, -0.04641077294945717, 0.041018784046173096, -0.004657063167542219, 0.07875178009271622, -0.026855938136577606, -0.058831676840782166, -0.04814939573407173, -0.027709433808922768, -0.09450532495975494, 0.008944518864154816, 0.02149580605328083, 0.043931618332862854, -0.056054215878248215, 0.00666089216247201, -0.03036065772175789, -0.01805971749126911, 0.04132357984781265, -0.038043681532144547, 0.03753799572587013, 0.06098207086324692, -0.05958077311515808, 0.006211778614670038, -0.0377286821603775, 0.08102760463953018, 0.1407376080751419, -0.14642776548862457, 0.03235289826989174, -0.061960965394973755, 0.04815404489636421, -0.062127284705638885, -4.789899676893583e-08, 0.02022721990942955, 0.05767684429883957, -0.008531815372407436, 0.027675427496433258, -0.04803323373198509, 0.01932121440768242, -0.07945907115936279, 0.02419489249587059, 0.04644855856895447, 0.06892954558134079, 0.11047149449586868, 0.07858437299728394, 0.08424871414899826, 0.029783276841044426, -0.03480739891529083, -0.05810150131583214, -0.02848701737821102, 0.01727350987493992, 0.016737719997763634, 0.004047672264277935, 0.04004419222474098, 0.03884965181350708, -0.024274617433547974, -0.05644415318965912, 0.08140373975038528, -0.04860542342066765, 0.001924289739690721, 0.09474801272153854, 0.04309239611029625, -0.04604485258460045, 0.012739702127873898, 0.03637167066335678, 0.025534695014357567, -0.013665836304426193, -0.018673602491617203, 0.013458284549415112, 0.023102987557649612, 0.0632457360625267, 0.029106339439749718, 0.001015818095766008, 0.005157229490578175, 0.05359829589724541, -0.02916271984577179, 0.03945528343319893, -0.09847584366798401, -0.028611892834305763, -0.08303872495889664, -0.03402744233608246, 0.06920763850212097, 0.019753364846110344, -0.08377861976623535, -0.029324617236852646, 0.0692477598786354, 0.048853203654289246, 0.014567403122782707, 0.031493689864873886, -0.017114469781517982, -0.01694577932357788, 0.00714076729491353, 0.05010318011045456, 0.06112146005034447, -0.08165579289197922, -0.11978878080844879, 0.01650514453649521]]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "print(embedding_function([character_split_texts[10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c63b61a-5c65-4d41-b492-65fcfd307f12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "934\n"
     ]
    }
   ],
   "source": [
    "print(len(character_split_texts[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2c57b342-a2b5-480a-9c3b-46ab4dc9c9f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13302"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chroma_client = chromadb.Client()\n",
    "chroma_collection = chroma_client.create_collection(\"IPCC_AR6_WGIII_FullReport.pdf\", embedding_function=embedding_function)\n",
    "\n",
    "ids = [str(i) for i in range(len(character_split_texts))]\n",
    "\n",
    "chroma_collection.add(ids=ids, documents=character_split_texts)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26d876df-cd6b-469e-9792-e67eec941a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2°C.  Nat. Clim. Change, 8(11), 931–933, doi:10.1038/s41558-018-0321-8.Mori, A.S., K.P . Lertzman, and L. Gustafsson, 2017: Biodiversity and ecosystem \n",
      "services in forest ecosystems: a research agenda for applied forest ecology. \n",
      "J. Appl. Ecol., 54(1), 12–27, doi:10.1111/1365-2664.12669.\n",
      "Moss, R. et al., 2016: Understanding dynamics and resilience in complex \n",
      "interdependent systems. Washington, DC, USA. https://www.globalchange.\n",
      "gov/browse/reports/understanding-dynamics-and-resilience-complex-\n",
      "interdependent-systems (Accessed July 23, 2021).\n",
      "Mugambiwa, S.S. and H.M.  Tirivangasi, 2017: Climate change: A  threat \n",
      "towards achieving ‘sustainable development goal number two’ (end \n",
      "hunger, achieve food security and improved nutrition and promote \n",
      "sustainable agriculture) in South Africa. Jamba J. Disaster Risk Stud., 9(1), \n",
      "a350, doi:10.4102/jamba.v9i1.350.\n",
      "Mulugetta, Y . and F . Urban, 2010: Deliberating on low carbon development.\n",
      "\n",
      "\n",
      "468\n",
      "Chapter 4 Mitigation and Development Pathways in the Near to Mid-term4\n",
      "In more recent years, some of these shifts in Brazil’s development \n",
      "pathways were undone. Political changes have redefined \n",
      "development priorities, with higher priority being given to \n",
      "agricultural development than climate change mitigation. The current \n",
      "administration has reduced the power of environmental agencies and \n",
      "forestry protection laws (including the forest code), while allowing \n",
      "the expansion of cropland to protected Amazon rainforest areas \n",
      "(Ferrante and Fearnside 2019; Rochedo et al. 2018). As a result, in \n",
      "2020, deforestation exceeded 11,000 km2, and reached the highest \n",
      "rate in the last 12 years (INPE 2020). The literature cautions that, \n",
      "if current policies and trends continue, the Amazon may reach an \n",
      "irreversible tipping point beyond which it will be impossible to \n",
      "remediate lost ecosystems and restore carbon sinks and indigenous \n",
      "people knowledge (Lovejoy and Nobre 2018; INPE 2019a; Nobre\n",
      "\n",
      "\n",
      "s10584-018-2189-z.\n",
      "Turnhout, E. et al., 2017: Envisioning REDD+ in a  post-Paris era: between \n",
      "evolving expectations and current practice. WIREs Clim. Change, 8(1), \n",
      "doi:10.1002/wcc.425.\n",
      "Turubanova, S., P .V. Potapov, A. Tyukavina, and M.C. Hansen, 2018: Ongoing \n",
      "primary forest loss in Brazil, Democratic Republic of the Congo, and Indonesia. \n",
      "Environ. Res. Lett., 13(7), 074028, doi:10.1088/1748-9326/aacd1c.\n",
      "Ünal, H.E., Ü.  Birben, and F .  Bolat, 2019: Rural population mobility, \n",
      "deforestation, and urbanization: case of Turkey. Environ. Monit. Assess., \n",
      "191(1), 21, doi:10.1007/s10661-018-7149-6.\n",
      "UNEP , 2019: Drivers of Environmental Change. In: Global Environment \n",
      "Outlook – GEO-6: Healthy Planet, Healthy People.\n",
      "UNEP , 2020: Emissions Gap Report 2020. United Nations Environment \n",
      "Programme (UNEP), Nairobi, Kenya, 112 pp.\n",
      "UNEP , 2021a: Becoming #GenerationRestoration: Ecosystem Restoration \n",
      "for People, Nature and Climate. United Nations Environment Programme, \n",
      "Nairobi, Kenya.56p.\n",
      "\n",
      "\n",
      "tropical deforestation emissions. Glob. Environ. Change, 56(March), 1–10, \n",
      "doi:10.1016/j.gloenvcha.2019.03.002.Perch, L., 2011: Mitigation of what and by what? Adaptation by whom and \n",
      "for whom? Dilemmas in delivering for the poor and the vulnerable in \n",
      "international climate policy., IPC-IG, Brasilia Brazil, https://www.ipc-undp.\n",
      "org/pub/IPCWorkingPaper79.pdf .\n",
      "Peres, R., E.  Muller, and V.  Mahajan, 2010: Innovation diffusion and new \n",
      "product growth models: A critical review and research directions. Int. J. Res. \n",
      "Mark., 27(2), 91–106, doi:10.1016/j.ijresmar.2009.12.012.\n",
      "Perez, C. et al., 2015: How resilient are farming households and communities \n",
      "to a changing climate in Africa? A gender-based perspective. Glob. Environ. \n",
      "Change, 34, 95–107, doi:10.1016/j.gloenvcha.2015.06.003.\n",
      "Perkins, P .E., 2019: Climate justice, commons, and degrowth. Ecol. Econ., 160, \n",
      "183–190, doi:10.1016/j.ecolecon.2019.02.005.\n",
      "Perkins, R., 2003: Environmental leapfrogging in developing countries: A critical\n",
      "\n",
      "\n",
      "feasibility of agricultural and forestry mitigation options, especially \n",
      "when deployed at large scale. Concern is greatest with widespread \n",
      "use of bioenergy crops, which could lead to forest losses (Harper \n",
      "et al. 2018). Deployment of BECCS and forest-based mitigation \n",
      "can be complementary (Favero et al. 2017; Baker et al. 2019), but \n",
      "inefficient policy approaches could lead to net carbon emissions if \n",
      "BECCS replaces high-carbon content ecosystems with crops.\n",
      "Adaptation benefits and biodiversity conservation. Biodiversity \n",
      "may improve resilience to climate change impacts as more-diverse \n",
      "systems could be more resilient to climate change impacts, thereby \n",
      "maintaining ecosystem function and preserving biodiversity (Hisano \n",
      "et al. 2018). However, losses in ecosystem functions due species \n",
      "shifts or reductions in diversity may impair the positive effects of \n",
      "biodiversity on ecosystems. Forest management strategies based on \n",
      "biodiversity and ecosystems functioning interactions can augment\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "claim = \"More than 75 percent of the rainforest is losing resilience; More than half of the rainforest could be converted into savanna in a matter of decades.\"\n",
    "query = \"More than 75 percent of the rainforest is losing resilience; More than half of the rainforest could be converted into savanna in a matter of decades.\"\n",
    "\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "for document in retrieved_documents:\n",
    "    print(document)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1535e6c-6e37-47ba-be8c-e0928e05afb0",
   "metadata": {},
   "source": [
    "### How to visualize the Embedding space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e48058b6-f757-4995-acd0-65b278effc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13302"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper_utils import word_wrap\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "\n",
    "# chroma_collection = load_chroma(filename='microsoft_annual_report_2022.pdf', collection_name='microsoft_annual_report_2022', embedding_function=embedding_function)\n",
    "chroma_collection.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85477b1f-051e-4caa-9f2e-9f01567555b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap\n",
    "# !pip install umap-learn\n",
    "# !pip install numpy scipy\n",
    "# !pip install scikit-learn\n",
    "# !pip install numba\n",
    "# !pip install umap-learn\n",
    "# !pip install tensorflow\n",
    "# !pip install pynndescent\n",
    "# !pip install docopt\n",
    "# !pip install gymnasium\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas\n",
    "# !pip install pygame\n",
    "# !pip install seaborn\n",
    "# !pip install tensorboardX\n",
    "# !pip install tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ddf9ed70-0ba7-45c1-998d-880d96c7cd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OMP: Info #276: omp_set_nested routine deprecated, please use omp_set_max_active_levels instead.\n"
     ]
    }
   ],
   "source": [
    "import umap\n",
    "# from umap import UMAP\n",
    "from umap import umap_ as UMAP\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "embeddings = chroma_collection.get(include=['embeddings'])['embeddings']\n",
    "# umap_transform = umap.UMAP(random_state=0, transform_seed=0).fit(embeddings)\n",
    "umap_transform = umap.UMAP().fit(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e02005c-f5af-4ebd-a6df-70d1a036a5a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_embeddings(embeddings, umap_transform):\n",
    "    umap_embeddings = np.empty((len(embeddings),2))\n",
    "    for i, embedding in enumerate(tqdm(embeddings)): \n",
    "        umap_embeddings[i] = umap_transform.transform([embedding])\n",
    "    return umap_embeddings   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ad2ffa-f471-4eea-9bf6-f03a6825de5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_dataset_embeddings = project_embeddings(embeddings, umap_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd25dc8b-c42f-48c5-b3d6-61a3c66502f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10)\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title('Projected Embeddings')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf494dc-7f3c-46af-8a17-2ef660879d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Relevancy and Distraction\n",
    "\n",
    "query = \"Is it still possible to limit warming to 1.5°C?\"\n",
    "\n",
    "results = chroma_collection.query(query_texts=query, n_results=5, include=['documents', 'embeddings'])\n",
    "\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "for document in results['documents'][0]:\n",
    "    print(document)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeadb116-497d-456a-9a5b-4a23ea4a763f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = embedding_function([query])[0]\n",
    "retrieved_embeddings = results['embeddings'][0]\n",
    "\n",
    "projected_query_embedding = project_embeddings([query_embedding], umap_transform)\n",
    "projected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa44084a-0d22-4fe1-a405-7c451ec1b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the projected query and retrieved documents in the embedding space\n",
    "plt.figure()\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\n",
    "plt.scatter(projected_query_embedding[:, 0], projected_query_embedding[:, 1], s=150, marker='X', color='r')\n",
    "plt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title(f'{query}')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a341520-12c3-4e3e-99c8-e7dbd74fcbd6",
   "metadata": {},
   "source": [
    "### How to use LLM to enhance query?\n",
    "\n",
    "Expansion with generated answers https://arxiv.org/abs/2305.03653"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574512f3-f75e-4776-b604-211340dd34c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_query_generated(query, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful research assistant. Provide an example answer to the given question, that might be found in a document like IPCC report. \"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": query}\n",
    "    ] \n",
    "\n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d7a00e-99f4-42e6-851b-b3f8f5864db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_query = \"Is it still possible to limit warming to 1.5°C?\"\n",
    "hypothetical_answer = augment_query_generated(original_query)\n",
    "\n",
    "joint_query = f\"{original_query} {hypothetical_answer}\"\n",
    "print(joint_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86d4b53-da6f-4c4c-bbb1-10f8c1f63ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = chroma_collection.query(query_texts=joint_query, n_results=5, include=['documents', 'embeddings'])\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "for doc in retrieved_documents:\n",
    "    print(doc)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23927005-c40e-45cf-a752-358f41f3a7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieved_embeddings = results['embeddings'][0]\n",
    "original_query_embedding = embedding_function([original_query])\n",
    "augmented_query_embedding = embedding_function([joint_query])\n",
    "\n",
    "projected_original_query_embedding = project_embeddings(original_query_embedding, umap_transform)\n",
    "projected_augmented_query_embedding = project_embeddings(augmented_query_embedding, umap_transform)\n",
    "projected_retrieved_embeddings = project_embeddings(retrieved_embeddings, umap_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3be24-25b6-4c87-b073-f85677c58e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the projected query and retrieved documents in the embedding space\n",
    "plt.figure()\n",
    "plt.scatter(projected_dataset_embeddings[:, 0], projected_dataset_embeddings[:, 1], s=10, color='gray')\n",
    "plt.scatter(projected_retrieved_embeddings[:, 0], projected_retrieved_embeddings[:, 1], s=100, facecolors='none', edgecolors='g')\n",
    "plt.scatter(projected_original_query_embedding[:, 0], projected_original_query_embedding[:, 1], s=150, marker='X', color='r')\n",
    "plt.scatter(projected_augmented_query_embedding[:, 0], projected_augmented_query_embedding[:, 1], s=150, marker='X', color='orange')\n",
    "\n",
    "plt.gca().set_aspect('equal', 'datalim')\n",
    "plt.title(f'{original_query}')\n",
    "plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4466f0c4-2b33-4e6a-a6fd-5b4e0cdb3733",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Fact-checking paper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f719b9-67ff-4637-8996-133cc1c08724",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### How to setup ChatGPT + IPCC (Advocate A)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8825d46a-d11c-4372-bdc8-536a7aa04e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdictsClimateFeedback = ['incorrect', 'inaccurate', 'imprecise', 'misleading','flawed_reasoing','lacks_context','unsupported','correct_but','mostly_correct','mostly_accurate','accurate', 'correct']\n",
    "# verdictsClimateFeedback = ['incorrect', 'imprecise', 'misleading','flawed_reasoing','unsupported','mostly_correct', 'correct']\n",
    "# verdictsClimateFeedback = ['incorrect', 'misleading','flawed_reasoing','unsupported', 'correct']\n",
    "# verdictsClimateFeedback = ['incorrect', 'correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e3eeb61e-9431-4f4c-bf2c-517a90d3878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LangChain's RetrievalQA, to associate Llama with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "def get_Advocate_A(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "            You are a Q&A bot , an intelligent system that acts as\n",
    "            a scientific fact - checker with vast knowledge of\n",
    "            climate change , climate science ,\n",
    "            environmental science , physics , and energy science .\n",
    "            You have been designed to answer users ’ questions\n",
    "            based on the information provided\n",
    "            above the question (the question is always in the last\n",
    "            line ) and your in - house knowledge .\n",
    "            You will be presented a claim , or a list of subclaims\n",
    "            that make up a whole claim .\n",
    "            Objective : Evaluate the accuracy of each of the user\n",
    "            statements solely based on the information\n",
    "            provided above each statement . In the end ,\n",
    "            aggregate the evaluation for each of the\n",
    "            subclaim to an overall statement about the veracity of\n",
    "            the claim .\n",
    "            Guidelines :\n",
    "            To ensure the most precise and comprehensive responses\n",
    "            , please follow the guidelines below :\n",
    "            1. Always base your verdict on the majority of the\n",
    "            information if conflicting evidence exists .\n",
    "            2. Do not rely solely on external sources or prior\n",
    "            knowledge . Use as much of the provided\n",
    "            information as possible to give a comprehensive\n",
    "            answer . If certain details are relevant , ensure\n",
    "            they are included in your response .\n",
    "            3. The user ’s question is ALWAYS in the final line .\n",
    "            When referencing the additional information above\n",
    "            the question , always cite the ’Reference ’, ’Page\n",
    "            ’, and ’URL ’. These details can be found below\n",
    "            each piece of information .\n",
    "            4. If there is insufficient information to answer a\n",
    "            question , reply with ’I cannot answer your\n",
    "            question ’\n",
    "            5. It is important to maintain accuracy and avoid\n",
    "            creating information . If any aspect is unclear ,\n",
    "            seek clarification from the respective chatbots .\n",
    "            Assessment process\n",
    "            1. Evaluate evidence and agreement\n",
    "            2. Synthesize finding and assess confidence (\n",
    "            qualitative judgment )\n",
    "            3. Quantify uncertainty with a likelihood assessment\n",
    "            when necessary and where possible ( requires\n",
    "            sufficient confidence ; uncertainty is not always\n",
    "            quantifiable ).\n",
    "            4. In your assessment , make three levels of evidence\n",
    "            and agreement : a) high b) medium c) low\n",
    "            Instructions on extreme claims\n",
    "            While there may be sources or projections supporting\n",
    "            a given claim ,\n",
    "            it ’s essential to discern if it represents a\n",
    "            consensus or an outlier viewpoint .\n",
    "            Provide a comprehensive evaluation that weighs both\n",
    "            the factual basis of the claim\n",
    "            and the potential for it being presented in an\n",
    "            exaggerated or misleading manner . Of course ,\n",
    "            extremes\n",
    "            can happen , but it should be clear that these are\n",
    "            extreme scenarios .\n",
    "            Response Format :\n",
    "            1. If you have not enough information , state that you\n",
    "            cannot assess the claim and return \"Not Enough\n",
    "            Information \" and stop further analysis .\n",
    "            2. Offer a detailed explanation for your verdict ,\n",
    "            including references to the ’Reference ’, ’Page ’,\n",
    "            and ’URL ’ when citing the provided information .\n",
    "            3. Specify the level of certainty in your assessmen by\n",
    "            stating the level of evidence and agreement . low\n",
    "            evidence and low agreement correspond to very\n",
    "            low uncertainty ,\n",
    "            high evidence and high agreement .\n",
    "            4. If you have enough information , provide verdict\n",
    "            from the following options at the end of your\n",
    "            explanation . Strictly follow the format of\n",
    "            encapsulating your verdict in two parathesis and\n",
    "            only use the following options :\"\"\" + str(verdictsClimateFeedback )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Information: {information}, \\n Question: {query}.\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94368852-44d8-4c2a-8208-2c832a972545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The claim that more than 75 percent of the rainforest is losing resilience and that more than half of the rainforest could be converted into savanna in a matter of decades is partially supported by the information provided above. \n",
      "\n",
      "The information mentions that recent political changes in Brazil have shifted development priorities, giving higher importance to agricultural development over climate change mitigation. This has led to reduced power of environmental agencies, changes in forestry protection laws, and the expansion of cropland into protected Amazon rainforest areas, resulting in a significant increase in deforestation rates. It is cautioned that if current policies and trends continue, the Amazon rainforest may reach an irreversible tipping point, making it impossible to remediate lost ecosystems and restore carbon sinks.\n",
      "\n",
      "While the provided information highlights significant threats to the Amazon rainforest due to deforestation and policy changes in Brazil, it does not explicitly quantify the exact percentage of rainforest areas that are losing resilience or could be converted into savanna. Therefore, there is insufficient direct evidence to support the specific percentages mentioned in the claim.\n",
      "\n",
      "Overall, the claim contains elements of truth based on the challenges faced by the Amazon rainforest, but the specific percentages lack direct support from the information provided.\n",
      "\n",
      "(unsupported)\n"
     ]
    }
   ],
   "source": [
    "# question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "query = \"More than 75 percent of the rainforest is losing resilience; More than half of the rainforest could be converted into savanna in a matter of decades.\"\n",
    "\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "# for document in retrieved_documents:\n",
    "#     print(document)\n",
    "#     print('\\n')\n",
    "\n",
    "output_A = get_Advocate_A(query=query, retrieved_documents=retrieved_documents)\n",
    "\n",
    "print(output_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "62825043-17e2-4f66-909d-6c3797703577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results['documents'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63769a",
   "metadata": {},
   "source": [
    "### How to setup ChatGPT-4 (Advocate B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "855d8062-4a40-45e9-802a-c4242fe19ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdictsClimateFeedback = ['incorrect', 'inaccurate', 'imprecise', 'misleading','flawed_reasoing','lacks_context','unsupported','correct_but','mostly_correct','mostly_accurate','accurate', 'correct']\n",
    "# verdictsClimateFeedback = ['incorrect', 'imprecise', 'misleading','flawed_reasoing','unsupported','mostly_correct', 'correct']\n",
    "# verdictsClimateFeedback = ['incorrect', 'misleading','flawed_reasoing','unsupported', 'correct']\n",
    "# verdictsClimateFeedback = ['incorrect', 'correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c18aee34-f398-4d45-9cee-67f521192246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LangChain's RetrievalQA, to associate Llama with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "def get_Advocate_B(query, model=\"gpt-4\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "        You are a Q&A bot , an intelligent system that acts as\n",
    "        a scientific fact - checker with vast knowledge of\n",
    "        climate change , climate science ,\n",
    "        environmental science , physics , and energy science .\n",
    "        You will be presented a claim , or a list of subclaims\n",
    "        that make up a whole claim .\n",
    "        Objective : Evaluate the accuracy of each of the user\n",
    "        statements . In the end , aggregate the evaluation\n",
    "        for each of the\n",
    "        subclaim to an overall statement about the veracity of\n",
    "        the claim .\n",
    "        Guidelines :\n",
    "        To ensure the most precise and comprehensive responses\n",
    "        , please follow the guidelines below :\n",
    "        1. Always base your verdict on the majority of the\n",
    "        information if conflicting evidence exists .\n",
    "        2. Do not rely solely on external sources or prior\n",
    "        knowledge . Use as much of the provided\n",
    "        information as possible to give a comprehensive\n",
    "        answer . If certain details are relevant , ensure\n",
    "        they are included in your response .\n",
    "        3. Your answer is always based in - house knowledge ,\n",
    "        indicate this by appending \"(In - house knowledge )\"\n",
    "        instead of providing a specific reference .\n",
    "        5. If there is insufficient information to answer a\n",
    "        question , reply with ’I cannot answer your\n",
    "        question ’\n",
    "        6. It is important to maintain accuracy and avoid\n",
    "        creating information . If any aspect is unclear ,\n",
    "        seek clarification from the respective chatbots .\n",
    "        Assessment process\n",
    "        1. Evaluate evidence and agreement\n",
    "        2. Synthesize finding and assess confidence (\n",
    "        qualitative judgment )\n",
    "        3. Quantify uncertainty with a likelihood assessment\n",
    "        when necessary and where possible ( requires\n",
    "        sufficient confidence ; uncertainty is not always\n",
    "        quantifiable ).\n",
    "        4. In your assessment , make three levels of evidence\n",
    "        and agreement : a) high b) medium c) low\n",
    "        Instructions on extreme claims\n",
    "        While there may be sources or projections supporting\n",
    "        a given claim ,\n",
    "        it ’s essential to discern if it represents a\n",
    "        consensus or an outlier viewpoint .\n",
    "        Provide a comprehensive evaluation that weighs both\n",
    "        the factual basis of the claim\n",
    "        and the potential for it being presented in an\n",
    "        exaggerated or misleading manner . Of course ,\n",
    "        extremes\n",
    "        can happen , but it should be clear that these are\n",
    "        extreme scenarios .\n",
    "        Response Format :\n",
    "        1. If you have not enough information , state that you\n",
    "        cannot assess the claim and return \"Not Enough\n",
    "        Information \" and stop further analysis .\n",
    "        2. Offer a detailed explanation for your verdict ,\n",
    "        including references to the ’Reference ’, ’Page ’,\n",
    "        and ’URL ’ when citing the provided information .\n",
    "        3. Specify the level of certainty in your assessmen by\n",
    "        stating the level of evidence and agreement . low\n",
    "        evidence and low agreement correspond to very\n",
    "        low uncertainty ,\n",
    "        high evidence and high agreement .\n",
    "        4. If you have enough information , provide verdict\n",
    "        from the following options at the end of your\n",
    "        explanation . Strictly follow the format of\n",
    "        encapsulating your verdict in two parathesis and\n",
    "        only use the following options :\"\"\" + str(verdictsClimateFeedback )\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}.\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfd1ece9-704e-49a0-b509-1ec9246a3b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's analyze these two subclaims individually:\n",
      "\n",
      "1) \"More than 75 percent of the rainforest is losing resilience.\"\n",
      "\n",
      "The resilience of a rainforest can indeed be affected by multiple factors such as deforestation, climate change, and wildfires. Nonetheless, the statement gives a specific percentage (75%) without providing accurate information. Resilience can be a difficult concept to quantify on a global level. While there is a significant amount of research providing evidence of rainforest vulnerability, a precise figure like 75% demands more concrete data.\n",
      "\n",
      "Assessment: medium evidence and low agreement due to the lack of precise quantification.\n",
      "\n",
      "2) \"More than half of the rainforest could be converted into savanna in a matter of decades.\"\n",
      "\n",
      "The conversion of rainforest into savanna is a real risk, but the timeline of \"a matter of decades\" could be misleading. It's broadly agreed in the scientific community that climate change, deforestation and other human activities are making rainforests susceptible to becoming savannas. However, the timescale is uncertain, with projections varying greatly.\n",
      "\n",
      "Assessment: medium evidence and medium agreement depending on the timeframe considered.\n",
      "\n",
      "Overall, it's important to confirm these claims with more concrete data. Modelling future ecosystem changes depends on several variables, some of which are unpredictable. While it's clear that rainforests are under threat, the specific percentages and timelines mentioned are not sufficiently supported, which can make the claim misleading despite its partially correct premise. (In-house knowledge)\n",
      "\n",
      "Overall Verdict: ('misleading')\n"
     ]
    }
   ],
   "source": [
    "query = \"More than 75 percent of the rainforest is losing resilience; More than half of the rainforest could be converted into savanna in a matter of decades.\"\n",
    "\n",
    "# results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "# retrieved_documents = results['documents'][0]\n",
    "\n",
    "output_B = get_Advocate_B(query=query)\n",
    "\n",
    "print(output_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dc60d2-c873-4bcb-bc6e-6f2303ad7f30",
   "metadata": {},
   "source": [
    "### How to setup Moderator?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53682ba9-8675-4a8a-b66c-78916dcfd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "verdictsClimateFeedback = ['incorrect', 'inaccurate', 'imprecise', 'misleading','flawed_reasoing','lacks_context','unsupported','correct_but','mostly_correct','mostly_accurate','accurate', 'correct']\n",
    "# verdictsClimateFeedback = ['incorrect', 'imprecise', 'misleading','flawed_reasoing','unsupported','mostly_correct', 'correct']\n",
    "# verdictsClimateFeedback = ['incorrect', 'misleading','flawed_reasoing','unsupported', 'correct']\n",
    "# verdictsClimateFeedback = ['incorrect', 'correct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "82820685-e0b9-411a-a93a-82020c6dbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use LangChain's RetrievalQA, to associate Llama with the loaded documents stored in the vector db\n",
    "from langchain.chains import RetrievalQA\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "def get_Moderator(output_A, output_B, model=\"gpt-4\"):\n",
    "    # information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "        Role : Authoritative Climate Scientist \" Arbitrator \"\n",
    "        System\n",
    "        Expertise : Climate change , climate science ,\n",
    "        environmental science , physics , energy science ,\n",
    "        and , most importantly , science communication\n",
    "        Primary Objective : Synthesize the assessment of the\n",
    "        veracity of a user ’s claim provided by Language\n",
    "        Model Modules , which we call Advocates .\n",
    "        Each Advocate operates based on different\n",
    "        authoritative documents . Note that all documents\n",
    "        are trustworty .\n",
    "        Arbitrator ’s Responsibilities :\n",
    "        1. Review : Examine the verdicts and explanations from\n",
    "        each LLM .\n",
    "        2. Consolidation : Determine the final verdict by\n",
    "        amalgamating the subclaims and LLM outputs .\n",
    "        3. Clarification : In case of discrepancies among LLMs ,\n",
    "        seek further evidence or explanations by asking\n",
    "        follow -up questions to the Advocates .\n",
    "        4. Lack of Evidence : If discrepancies surface among\n",
    "        the LLM verdicts , prioritize the judgments of\n",
    "        LLMs that provide specific information for claim\n",
    "        assessment\n",
    "        over those that don ’t. For illustration , if the\n",
    "        majority of LLMs neither support nor contradict a\n",
    "        claim due to lack of specific information ,\n",
    "        but one LLM provides evidence either in favor of or\n",
    "        against the claim , then lean towards that\n",
    "        particular LLM ’s verdict.However , clearly note that such evidence isn ’t\n",
    "        broadly substantiated or compelling .\n",
    "        5. Information Source : Primarily use the provided data\n",
    "        . Avoid over - relying on external sources or\n",
    "        previous knowledge .\n",
    "        Final Assessment Criteria :\n",
    "        1. Analysis : Contemplate the collective LLM assessment\n",
    "        .\n",
    "        2. Holistic Judgment : Don ’t solely depend on majority\n",
    "        verdicts . Do not consider Advocates that say that\n",
    "        they have not enough information .\n",
    "        Consider the importance of each evaluation and any\n",
    "        disparities .\n",
    "        3. Confidence Hierarchy : Treat IPCC , WMO , and 1000\n",
    "        _scientists with higher regard than CC_abstracts .\n",
    "        Guidelines :\n",
    "        1. If discrepancies arise predominantly because there\n",
    "        is \"not enough information \" or a Advocate finds\n",
    "        the claim \" unsupported \" due to lack of\n",
    "        information in the provided information , but one or\n",
    "        more Advocates find clear evidence either\n",
    "        supporting or refuting the claim , prioritize\n",
    "        those with concrete evidence .\n",
    "        2. When to ask Follow -up Questions If Advocates reach\n",
    "        contrasting conclusions based on differing pieces\n",
    "        of evidence they possess ,\n",
    "        suggest a set of follow -up questions or prompts\n",
    "        that would clarify the inconsistencies and\n",
    "        further the debate . If you are uncertain about\n",
    "        making a final verdict , go for a debating round .\n",
    "        3. Stop asking Follow -up Questions : If no Advocate\n",
    "        changes it ’s assessment , close the debating round\n",
    "        , no follow up questions , and state the final\n",
    "        verdict .\n",
    "        4. User Questions : Always located at the bottom . Cite\n",
    "        ’Reference ’, ’Page ’, and ’URL ’ when referring to\n",
    "        data above the question .\n",
    "        5. Source of Answer : If a response is from in - house\n",
    "        knowledge , append (In - house knowledge ).\n",
    "        Assessment process\n",
    "        1. Evaluate evidence and agreement\n",
    "        2. Synthesize finding and assess confidence (\n",
    "        qualitative judgment )\n",
    "        3. Quantify uncertainty with a likelihood assessment\n",
    "        when necessary and where possible ( requires\n",
    "        sufficient confidence ; uncertainty is not always\n",
    "        quantifiable ).\n",
    "        4. In your assessment , make three levels of evidence\n",
    "        and agreement : a) high b) medium c) low\n",
    "        Final Line of the Output\n",
    "        1. State again the original user query .\n",
    "        2. Then provide a summary of the verdict for the\n",
    "        user_query by aggregating the subclaims (if there\n",
    "        were any ) and explain your reasoning in\n",
    "        accessible language .\n",
    "        3. Do not make a verdict based on majority voting if\n",
    "        there is disagreement . Instead , all the Advocates\n",
    "        verdict should be close .\n",
    "        3. If there is not full agreement , then ask follow -\n",
    "        upquestions for a next round of debate .\n",
    "        4. If you have enough convergent information based on\n",
    "        the Advocates information , provide verdict from\n",
    "        the following options at the end of your\n",
    "        explanation . Strictly follow the format of\n",
    "        encapsulating your verdict in two parathesis and\n",
    "        only use the following options :\"\"\" + str(verdictsClimateFeedback ) + f\"\"\"5. Stricly use the information provided by the\n",
    "        Advocates .\n",
    "        \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"AdvocateA: {output_A}, AdvocateB: {output_B}.\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d216c884-aa39-4212-8a9f-a2297fd780cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have reviewed the assessments provided by both AdvocateA and AdvocateB. \n",
      "\n",
      "AdvocateA informs us that while the data provided does highlight the threat to the Amazon rainforest due to increased deforestation and changes in regulations, the exact percentages noted in the claim aren't directly supported by the data. This lack of exactness can be considered as not fully supporting the claim. \n",
      "\n",
      "AdvocateB, taking a slightly different approach, analyses the two subclaims individually. They emphasize that resilience can indeed be affected by various factors but giving a specific percentage like 75% without accurate information does not make it trustworthy. The second part of the claim concerning conversion into savanna they note is a real risk but the timeline of \"a matter of decades\" could potentially be misleading because projections vary greatly. Thus, it is stated that a claim can be misleading despite its partially correct premise.\n",
      "\n",
      "Despite the aforementioned, it's clear that the specific percentages and timeline given are not fully supported by the provided information. Therefore, based on the collective assessment and synthesized agreement, it can be concluded that the user's claim that \"More than 75 percent of the rainforest is losing resilience and more than half of the rainforest could be converted into savanna in a matter of decades\" is misleading because although it represents true concerns and threats, the data provided does not specifically support the percentages and timeline proposed in the claim. \n",
      "\n",
      "Thus, the final verdict for this assertion is: ('misleading').\n"
     ]
    }
   ],
   "source": [
    "query = \"More than 75 percent of the rainforest is losing resilience; More than half of the rainforest could be converted into savanna in a matter of decades.\"\n",
    "\n",
    "output_final = get_Moderator(output_A, output_B)\n",
    "\n",
    "print(output_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbcc1a5-c46d-4b0f-bf1d-5b4a3cf22172",
   "metadata": {},
   "source": [
    "## LLM + Sea Level Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86fb47cc-190e-49c0-ad73-23faa007dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.agents import tool\n",
    "from langchain.tools import BaseTool, StructuredTool, tool\n",
    "from langchain.pydantic_v1 import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba7c9bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "233d0295-4dd7-49cf-a0e0-c05d9f14c870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# from pydantic import BaseModel, Field\n",
    "import datetime\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    Year: int = Field(..., description=\"number of Year for the global mean sea level prediction\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_sea_level_prediction(Year: int, setting = None) -> list:\n",
    "    \"\"\"Fetch prediction of global mean sea level for given a Year.\"\"\"\n",
    "    \n",
    "    df = pd.read_csv('GMSL_prediction_SEM.csv')\n",
    "    mean_no = df.iloc[Year-1950, 1]\n",
    "    up95_no = df.iloc[Year-1950, 2]\n",
    "    low95_no = df.iloc[Year-1950, 3]\n",
    "    \n",
    "    # return [mean_no, low95_no, up95_no]\n",
    "    return f\"The prediction of global mean sea level in {Year} is {mean_no}. The 95% of confidence interval is from {low95_no} to {up95_no}.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8b6e41e7-17f5-49ca-9ac4-8fd86c780148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_sea_level_prediction'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sea_level_prediction.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "dfb47b74-41b5-41da-8d55-4342e3227c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'get_sea_level_prediction(Year: int, setting=None) -> list - Fetch prediction of global mean sea level for given a Year.'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sea_level_prediction.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "46560dbd-c114-4349-95f7-de957288bd12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Year': {'title': 'Year',\n",
       "  'description': 'number of Year for the global mean sea level prediction',\n",
       "  'type': 'integer'}}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_sea_level_prediction.args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "7ac85b94-b394-415c-bd64-ead734487470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.render import format_tool_to_openai_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5993c775-f2cc-44ba-a0b0-046c3726169b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'get_sea_level_prediction',\n",
       " 'description': 'get_sea_level_prediction(Year: int, setting=None) -> list - Fetch prediction of global mean sea level for given a Year.',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'Year': {'description': 'number of Year for the global mean sea level prediction',\n",
       "    'type': 'integer'}},\n",
       "  'required': ['Year']}}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_tool_to_openai_function(get_sea_level_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "42d23f8c-9e4a-4232-a3e7-b9f8b89e1eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.chat_models import ChatOpenAI\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "f6191756-9628-438b-9d47-b80e340e8f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [\n",
    "    format_tool_to_openai_function(get_sea_level_prediction)\n",
    "]\n",
    "model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0).bind(functions=functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "52ce8948-12d2-4fb5-bfdf-6f2bc4b91a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2c50d6bb-8465-4b4d-900a-a1711c537c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "c1391fba-6e20-4479-bf43-8e7d099fa18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "def route(result):\n",
    "    if isinstance(result, AgentFinish):\n",
    "        return result.return_values['output']\n",
    "    else:\n",
    "        tools = {\n",
    "            \"get_sea_level_prediction\": get_sea_level_prediction,\n",
    "        }\n",
    "        return tools[result.tool].run(result.tool_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "bab26985-16c1-4ad2-a709-d97b16a38839",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser() | route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "392e6439-0774-4111-8e19-77ee4280f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence] Entering Chain run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Will coastal New York see the impact of Greenland ice sheet melting by 2100?\"\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] Entering Prompt run with input:\n",
      "\u001b[0m{\n",
      "  \"input\": \"Will coastal New York see the impact of Greenland ice sheet melting by 2100?\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 2:prompt:ChatPromptTemplate] [1ms] Exiting Prompt run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"prompts\",\n",
      "    \"chat\",\n",
      "    \"ChatPromptValue\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"messages\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"SystemMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"You are helpful but sassy assistant\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      },\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"HumanMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"Will coastal New York see the impact of Greenland ice sheet melting by 2100?\",\n",
      "          \"additional_kwargs\": {}\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[llm/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] Entering LLM run with input:\n",
      "\u001b[0m{\n",
      "  \"prompts\": [\n",
      "    \"System: You are helpful but sassy assistant\\nHuman: Will coastal New York see the impact of Greenland ice sheet melting by 2100?\"\n",
      "  ]\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[llm/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 3:llm:ChatOpenAI] [1.10s] Exiting LLM run with output:\n",
      "\u001b[0m{\n",
      "  \"generations\": [\n",
      "    [\n",
      "      {\n",
      "        \"text\": \"\",\n",
      "        \"generation_info\": {\n",
      "          \"finish_reason\": \"function_call\",\n",
      "          \"logprobs\": null\n",
      "        },\n",
      "        \"type\": \"ChatGeneration\",\n",
      "        \"message\": {\n",
      "          \"lc\": 1,\n",
      "          \"type\": \"constructor\",\n",
      "          \"id\": [\n",
      "            \"langchain\",\n",
      "            \"schema\",\n",
      "            \"messages\",\n",
      "            \"AIMessage\"\n",
      "          ],\n",
      "          \"kwargs\": {\n",
      "            \"content\": \"\",\n",
      "            \"additional_kwargs\": {\n",
      "              \"function_call\": {\n",
      "                \"arguments\": \"{\\\"Year\\\":2100}\",\n",
      "                \"name\": \"get_sea_level_prediction\"\n",
      "              }\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  ],\n",
      "  \"llm_output\": {\n",
      "    \"token_usage\": {\n",
      "      \"completion_tokens\": 18,\n",
      "      \"prompt_tokens\": 107,\n",
      "      \"total_tokens\": 125\n",
      "    },\n",
      "    \"model_name\": \"gpt-3.5-turbo\",\n",
      "    \"system_fingerprint\": \"fp_3bc1b5746c\"\n",
      "  },\n",
      "  \"run\": null\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:OpenAIFunctionsAgentOutputParser] Entering Parser run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 4:parser:OpenAIFunctionsAgentOutputParser] [1ms] Exiting Parser run with output:\n",
      "\u001b[0m{\n",
      "  \"lc\": 1,\n",
      "  \"type\": \"constructor\",\n",
      "  \"id\": [\n",
      "    \"langchain\",\n",
      "    \"schema\",\n",
      "    \"agent\",\n",
      "    \"AgentActionMessageLog\"\n",
      "  ],\n",
      "  \"kwargs\": {\n",
      "    \"tool\": \"get_sea_level_prediction\",\n",
      "    \"tool_input\": {\n",
      "      \"Year\": 2100\n",
      "    },\n",
      "    \"log\": \"\\nInvoking: `get_sea_level_prediction` with `{'Year': 2100}`\\n\\n\\n\",\n",
      "    \"message_log\": [\n",
      "      {\n",
      "        \"lc\": 1,\n",
      "        \"type\": \"constructor\",\n",
      "        \"id\": [\n",
      "          \"langchain\",\n",
      "          \"schema\",\n",
      "          \"messages\",\n",
      "          \"AIMessage\"\n",
      "        ],\n",
      "        \"kwargs\": {\n",
      "          \"content\": \"\",\n",
      "          \"additional_kwargs\": {\n",
      "            \"function_call\": {\n",
      "              \"arguments\": \"{\\\"Year\\\":2100}\",\n",
      "              \"name\": \"get_sea_level_prediction\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    ]\n",
      "  }\n",
      "}\n",
      "\u001b[32;1m\u001b[1;3m[chain/start]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:chain:route] Entering Chain run with input:\n",
      "\u001b[0m[inputs]\n",
      "\u001b[32;1m\u001b[1;3m[tool/start]\u001b[0m \u001b[1m[1:tool:get_sea_level_prediction] Entering Tool run with input:\n",
      "\u001b[0m\"{'Year': 2100}\"\n",
      "\u001b[36;1m\u001b[1;3m[tool/end]\u001b[0m \u001b[1m[1:tool:get_sea_level_prediction] [7ms] Exiting Tool run with output:\n",
      "\u001b[0m\"The prediction of global mean sea level in 2100 is 686.690760499435. The 95% of confidence interval is from 604.018417351968 to 769.363103646902.\"\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence > 5:chain:route] [8ms] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The prediction of global mean sea level in 2100 is 686.690760499435. The 95% of confidence interval is from 604.018417351968 to 769.363103646902.\"\n",
      "}\n",
      "\u001b[36;1m\u001b[1;3m[chain/end]\u001b[0m \u001b[1m[1:chain:RunnableSequence] [1.12s] Exiting Chain run with output:\n",
      "\u001b[0m{\n",
      "  \"output\": \"The prediction of global mean sea level in 2100 is 686.690760499435. The 95% of confidence interval is from 604.018417351968 to 769.363103646902.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# question = \"What is the global sea level prediction in 2025?\"\n",
    "question = \"Will coastal New York see the impact of Greenland ice sheet melting by 2100?\"\n",
    "result = chain.invoke({\"input\": question})\n",
    "# result = chain.invoke({\"input\": \"What is the weather in Stony Brook right now?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "b832ad53-8424-4623-b26e-70436db22708",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The prediction of global mean sea level in 2100 is 686.690760499435. The 95% of confidence interval is from 604.018417351968 to 769.363103646902.'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "74e0f49f-caa5-43eb-8c20-41c6aa795371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "def get_response(question, result, model=\"gpt-3.5-turbo\"):\n",
    "    information = result\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"\"\"\n",
    "            You are a Q&A bot , an intelligent system that acts as\n",
    "            a scientific fact - checker with vast knowledge of\n",
    "            climate change , climate science ,\n",
    "            environmental science , physics , and energy science .\n",
    "            You have been designed to answer users ’ questions\n",
    "            based on the information provided\n",
    "            above the question (the question is always in the last\n",
    "            line ) and your in - house knowledge .\n",
    "            You will be presented a sea level prediction question.\n",
    "            Objective : Answer the question based on the information provided.\n",
    "            \"\"\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Information: {information}, \\n Question: {question}.\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "025e6061-0577-4de9-9b03-19065eecb69b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Based on the provided sea level prediction for 2100 and the 95% confidence interval, coastal New York is likely to see the impact of Greenland ice sheet melting by 2100. The predicted global mean sea level rise of 686.690760499435 by 2100 falls within the 95% confidence interval of 604.018417351968 to 769.363103646902. This indicates a high probability of sea level rise that could result from the melting of the Greenland ice sheet, which would impact coastal areas like New York.'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_response(question, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ebb581-83fa-4ad7-8ac2-44159f993bcb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
