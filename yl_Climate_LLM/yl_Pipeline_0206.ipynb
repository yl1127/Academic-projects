{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a516e51-795a-4598-843f-36d8fd7f3221",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**New**: Pipeline details\n",
    "\n",
    "How does RAG(retrieval-augmented generation) work?\n",
    "        \n",
    "\n",
    "\n",
    "(1.31)New model: [Bard(Original)](https://bard.google.com/chat) For 13 questions from paper, [13 answers](https://g.co/bard/share/117570e457af) For 7 new questions, [7 answers](https://g.co/bard/share/644ebd6a4f50) API [Join Waitlist](https://cloud.google.com/ai/earlyaccess/join?hl=en)\n",
    "\n",
    "(1.22) View the all [answers](yl_Appendix.ipynb).\n",
    "\n",
    "(1.16) 7 questions:\n",
    "\n",
    "1. What is the best estimate of the equilibrium climate sensitivity?\n",
    "2. Is it possible the Arctic will become ice free at some points before 2050?\n",
    "3. How likely will the world see runaway ice loss from the Antarctic  ice sheets by 2100?\n",
    "4. To what extent has climate change impacted ice sheets in both polar regions?\n",
    "5. Will coastal New York see the impact of Greenland ice sheet melting by 2100?\n",
    "6. What will be the likely changes to weather and climate extremes in coastal New York around 2050 if the world take no action to reduce greenhouse gas emissions?\n",
    "7. What are the primary evidence and how robust is it that human activities caused the observed global warming since the pre-industrial period?\n",
    "\n",
    "The 13 original questions are from [chatClimate: Grounding Conversational AI in Climate Science](https://arxiv.org/abs/2304.05510)\n",
    "\n",
    "\n",
    "\n",
    "The Climate report is from [IPCC_AR6_WGIII_FullReport](https://sdgs.un.org/sites/default/files/2023-01/IPCC_AR6_WGIII_FullReport.pdf)\n",
    "\n",
    "(1.2)The Llama2(Original), ChatGPT-4(Original), Llama2+(Climate report) and Llama2 hybird code parts are available [here](https://github.com/yl1127/Academic-projects/blob/main/yl_Climate_LLM/yl_Climate_0102.ipynb).\n",
    "\n",
    "\n",
    "(2023.12.18)The Llama2(Original) and Llama2+(Climate report) code parts are available [here](https://github.com/yl1127/Academic-projects/blob/main/yl_Climate_LLM/yl_LlamaCloud_Climate.ipynb).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14baf22-8c82-499d-8371-fa991679d3a6",
   "metadata": {},
   "source": [
    "<!-- | Rate | Llama2(Original) | GPT-4(Original) | Llama2 +(Climate report)|Llama2 hybird|\n",
    "| ----------- | ----------- |------------------|-------------------------|-----------------------|\n",
    "|Q1: What is the best estimate of the equilibrium climate sensitivity?| -- | -- | -- |  --|\n",
    "|Q2: Is it possible the Arctic will become ice free at some points before 2050?| -- | -- | -- |  --|\n",
    "|Q3: How likely will the world see runaway ice loss from the Antarctic  ice sheets by 2100?| -- | -- | -- |  --|\n",
    "|Q4: To what extent has climate change impacted ice sheets in both polar regions?| -- | -- | -- |  --|\n",
    "|Q5: Will coastal New York see the impact of Greenland ice sheet melting by 2100?| -- | -- | -- |  --|\n",
    "|Q6: What will be the likely changes to weather and climate extremes in coastal New York around 2050 if the world take no action to reduce greenhouse gas emissions?| -- | -- | -- |  --|\n",
    "|Q7: What are the primary evidence and how robust is it that human activities caused the observed global warming since the pre-industrial period?| -- | -- | -- |  --| -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1ea03a-cc69-45b0-80d3-664e48ca6831",
   "metadata": {},
   "source": [
    "## This demo app shows:\n",
    "* How to run Llama2 in the cloud hosted on Replicate\n",
    "* How to use LangChain to ask Llama general questions and follow up questions\n",
    "* How to use LangChain to load a recent PDF doc - This is the well known RAG (Retrieval Augmented Generation) method to let LLM such as Llama2 be able to answer questions about the data not publicly available when Llama2 was trained, or about your own data. RAG is one way to prevent LLM's hallucination\n",
    "* How to use LangChain to load a directory which includes multiple PDF docs.\n",
    "* You should also review the [HelloLlamaLocal](HelloLlamaLocal.ipynb) notebook for more information on RAG\n",
    "\n",
    "**Note** We will be using Replicate to run the examples here. You will need to first sign in with Replicate with your github account, then create a free API token [here](https://replicate.com/account/api-tokens) that you can use for a while. \n",
    "After the free trial ends, you will need to enter billing info to continue to use Llama2 hosted on Replicate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dde626",
   "metadata": {},
   "source": [
    "Let's start by installing the necessary packages:\n",
    "- sentence-transformers for text embeddings\n",
    "- chromadb gives us database capabilities \n",
    "- langchain provides necessary RAG tools for this demo\n",
    "\n",
    "And setting up the Replicate token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd8508-c8b8-4cd2-be26-01b97e664ff9",
   "metadata": {},
   "source": [
    "### How to run original Llama2 and ChatGPT?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c608df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain replicate sentence-transformers chromadb pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edac4bba-800d-4ea5-b3a3-f4be2ae9f6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8870c1",
   "metadata": {},
   "source": [
    "Next we call the ChatGPT model from OpenAI. In this example we will use the ChatGPT-3.5-turbo chat model. You can find more ChatGPT models by searching for them on the [OpenAI API keys](https://platform.openai.com/api-keys).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f928862f-3003-4f86-ba2b-b13a11b1df61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " ········\n"
     ]
    }
   ],
   "source": [
    "# OpenAI API\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "# _ = load_dotenv(find_dotenv()) # read local .env file\n",
    "new_api_key = getpass()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = new_api_key\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd207c80",
   "metadata": {},
   "source": [
    "With the model set up, you are now ready to ask some questions. Here is an example of the simplest way to ask the model some general questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08366ebd-7ed5-4a88-8b11-0d5c79ed1127",
   "metadata": {},
   "source": [
    "### How to load an external PDF and do embedding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5303d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95254686-d140-4187-b9a4-fc39195e52dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working Group III contribution to the\n",
      "Sixth Assessment Report of the \n",
      "Intergovernmental Panel on Climate ChangeWGIIIMitigation of Climate ChangeClimate Change 2022\n"
     ]
    }
   ],
   "source": [
    "## Load the Report and show the first page content\n",
    "from pypdf import PdfReader\n",
    "loader = PdfReader(\"IPCC_AR6_WGIII_FullReport.pdf\")\n",
    "pdf_texts = [p.extract_text().strip() for p in loader.pages] # remove some empty contents\n",
    "pdf_texts = [text for text in pdf_texts if text]\n",
    "print(pdf_texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "678c2b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030\n",
      "Foreword  \n",
      "and Preface\n"
     ]
    }
   ],
   "source": [
    "## check docs length and content\n",
    "print(len(pdf_texts))\n",
    "print(pdf_texts[3]) # any pages between 0 to 2030(len(pdf_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8268e",
   "metadata": {},
   "source": [
    "We need to store our documents. There are more than 30 vector stores (DBs) supported by LangChain. \n",
    "For this example we will use [Chroma](https://python.langchain.com/docs/integrations/vectorstores/chroma) which is light-weight and in memory so it's easy to get started with.\n",
    "For other vector stores especially if you need to store a large amount of data - see https://python.langchain.com/docs/integrations/vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f0787375-c30c-468d-83ee-95e55a99a6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, SentenceTransformersTokenTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "853a3002-8987-41d2-83f5-7b2406aaa6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "approaches falling outside the scope of individual sectors. \n",
      "As in the AR5, there is a chapter on recent trends and drivers, with \n",
      "the scope expanded to cover historic emissions and recent policy \n",
      "developments. Following the pattern established in the WG III AR5 \n",
      "report, and the Special Report on Global Warming of 1.5°C, this \n",
      "report assesses published emission scenarios with a 21st century \n",
      "perspective. Modelled emission scenarios are categorised according \n",
      "to climate outcomes, allowing a handshake with the WG I assessment. \n",
      "To meet the goal of linking top-down and bottom-up insights, the \n",
      "report includes an additional pathways chapter that provides a mid-\n",
      "century perspective, focussing on national and regional scales and the \n",
      "alignment between development pathways and mitigation actions.\n",
      "As in the AR5, this report addresses mitigation enablers such as \n",
      "international cooperation, finance and investment, and policies and\n",
      "\n",
      "Total chunks: 13302\n"
     ]
    }
   ],
   "source": [
    "## Split the report as small pieces(chunks)\n",
    "character_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20\n",
    ") # Each pieces has 1000 unit length\n",
    "character_split_texts = character_splitter.split_text('\\n\\n'.join(pdf_texts))\n",
    "\n",
    "print(character_split_texts[10])\n",
    "print(f\"\\nTotal chunks: {len(character_split_texts)}\") # Total number of small chunks(13302)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d4a17c",
   "metadata": {},
   "source": [
    "To store the documents, we will need to split them into chunks using [`RecursiveCharacterTextSplitter`](https://python.langchain.com/docs/modules/data_connection/document_transformers/text_splitters/recursive_text_splitter) and create vector representations of these chunks using [`HuggingFaceEmbeddings`](https://www.google.com/search?q=langchain+hugging+face+embeddings&sca_esv=572890011&ei=ARUoZaH4LuumptQP48ah2Ac&oq=langchian+hugg&gs_lp=Egxnd3Mtd2l6LXNlcnAiDmxhbmdjaGlhbiBodWdnKgIIADIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCjIHEAAYgAQYCkjeHlC5Cli5D3ABeAGQAQCYAV6gAb4CqgEBNLgBAcgBAPgBAcICChAAGEcY1gQYsAPiAwQYACBBiAYBkAYI&sclient=gws-wiz-serp) on them before storing them into our vector database. \n",
    "\n",
    "In general, you should use larger chuck sizes for highly structured text such as code and smaller size for less structured text. You may need to experiment with different chunk sizes and overlap values to find out the best numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "47cb6410-ac1d-4f01-8b90-5a4fce754798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0008131897775456309, 0.005816067568957806, 0.07490230351686478, 0.06837400794029236, 0.08766964823007584, -0.0012497920542955399, -0.0640428364276886, -0.041942812502384186, -0.07316127419471741, 0.01825740560889244, -0.08234508335590363, -0.05028411000967026, -0.03627554699778557, 0.057594843208789825, -0.026721015572547913, 0.020879218354821205, -0.11043477058410645, -0.0446099191904068, -0.06738457083702087, -0.09205488860607147, 0.025163700804114342, 0.053215134888887405, 0.034258224070072174, -0.010387456975877285, 0.018884461373090744, -0.05462275445461273, -0.025599760934710503, 0.023365968838334084, -0.06750442087650299, 0.08059892803430557, 0.0327172689139843, 0.08507392555475235, -0.0669960305094719, -0.016989467665553093, 0.02977531962096691, 0.0656718760728836, 0.03229948505759239, 0.01954924501478672, 0.03471197932958603, 0.004731947090476751, 0.034873683005571365, -0.08469009399414062, 0.0052584330551326275, -0.05337139219045639, -0.03895178809762001, 0.023106735199689865, 0.02990221418440342, -0.029597017914056778, -0.09657876193523407, -0.03529026731848717, 0.045329999178647995, 0.009732252918183804, -0.028350260108709335, -0.037561431527137756, 0.01669039949774742, -0.0825960636138916, 0.024609863758087158, -0.03386129066348076, 0.03059246577322483, -0.024605877697467804, 0.040210284292697906, -0.0997571349143982, -0.05041476711630821, -0.03270818665623665, 0.013197485357522964, 0.029560504481196404, 0.02082779072225094, 0.05168429762125015, -0.0803096666932106, 0.022983353585004807, -0.015998199582099915, -0.05991418659687042, -0.037617895752191544, -0.10007444769144058, -0.0456216037273407, -0.02886275015771389, 0.041077323257923126, 0.06851901859045029, 0.136552631855011, -0.06198299303650856, 0.07946737855672836, 0.05629274621605873, 0.04570234566926956, -0.014305179938673973, -0.009163088165223598, 0.015942394733428955, 0.02562364935874939, -0.055027373135089874, 0.01518801786005497, 0.010028722696006298, -0.08051285892724991, -0.10780772566795349, 0.11219830065965652, 0.06828782707452774, 0.009419404901564121, 0.049908142536878586, 0.05250187963247299, -0.07206862419843674, -0.03957436978816986, -0.025236407294869423, 0.05405259132385254, 0.031207280233502388, -0.031230628490447998, -0.014256677590310574, -0.04518303647637367, -0.04755416512489319, 0.03349408134818077, -0.018141144886612892, -0.0028857975266873837, -0.08269155770540237, -0.020513346418738365, -0.003896430367603898, 0.03097221441566944, -0.10416177660226822, -0.00555293308570981, -0.030283940955996513, 0.054701123386621475, -0.034529104828834534, 0.006761545781046152, -0.05741405114531517, -0.053610071539878845, 0.02948353812098503, -0.008551356382668018, 0.033117879182100296, 0.027616068720817566, 0.0009091644897125661, -0.02234608121216297, -3.148199557233894e-34, -0.02378743514418602, 0.04083823040127754, -0.024610130116343498, 0.0558808296918869, -0.027471652254462242, 0.010531491599977016, -0.03506104275584221, -0.021030627191066742, -0.04993145540356636, 0.02193526178598404, 0.03797470033168793, 0.0331217497587204, -0.015208247117698193, 0.0445709191262722, 0.09951749444007874, -0.03827712684869766, -0.06261850148439407, 0.07422152906656265, -0.004499465227127075, 0.04742663353681564, -0.019497670233249664, -0.043509989976882935, 0.0515407919883728, 0.004180459305644035, 0.05637998878955841, 0.05531167984008789, 0.0801117792725563, -0.03806519880890846, -0.03197341039776802, 0.024994639679789543, 0.03321276605129242, 0.06589791178703308, -0.04928842559456825, -0.022285422310233116, -0.030548593029379845, 0.004628104157745838, -0.05025928094983101, 0.0004208416794426739, -0.05127362906932831, 0.03598640114068985, -0.01865704357624054, 0.04023595154285431, -0.06646502763032913, -0.02033206820487976, 0.08865194767713547, 0.053962673991918564, 0.058790676295757294, -0.0017052609473466873, -0.04289250448346138, -0.012654359452426434, -0.06561136245727539, 0.04359026625752449, -0.011275201104581356, -0.1174880862236023, 0.007202399428933859, -0.015101270750164986, 0.012693339958786964, -0.06239952892065048, -0.022247925400733948, -0.04646492376923561, -0.007934137247502804, -0.04324409365653992, -0.04633884131908417, 0.014951717108488083, -0.009135446511209011, 0.07956352829933167, 0.0043992879800498486, 0.013315578922629356, 0.007010726723819971, 0.0439438596367836, 0.013059110380709171, -0.02631811611354351, 0.05496983602643013, 0.052906692028045654, 0.03567368909716606, 0.036080677062273026, 0.026905281469225883, 0.05146678909659386, 0.028133627027273178, 0.034605465829372406, -0.09991006553173065, 0.009389741346240044, 0.020398978143930435, -0.10734456777572632, 0.014839605428278446, -0.06565368920564651, 0.0353374108672142, 0.02838645689189434, 0.04385439306497574, -0.0007286438485607505, -0.12002497166395187, -0.006314493715763092, 0.026596929877996445, 0.04323914647102356, -0.02157946489751339, -1.3903654427674897e-33, 0.08855465054512024, 0.03472083806991577, 0.005496592726558447, -0.0422530472278595, -0.10618708282709122, -0.040631745010614395, -0.01252724602818489, -0.08295300602912903, 0.04795144125819206, 0.01763846166431904, -0.001118838437832892, 0.01615477353334427, 0.06258918344974518, 0.053968094289302826, 0.005201802588999271, -0.08168451488018036, 0.004934568889439106, -0.06701386719942093, 0.041503164917230606, -0.018674196675419807, 0.07067793607711792, -0.12410940229892731, -0.022031666710972786, 0.05769237503409386, -0.04759272560477257, 0.062187422066926956, 0.04384222626686096, -0.0801955834031105, 0.0660632997751236, -0.06978011131286621, -0.07979913055896759, 0.059198133647441864, -0.042320046573877335, 0.01240536104887724, -0.07576002925634384, 0.009405973367393017, 0.0722941979765892, -0.06410612165927887, -0.060173869132995605, -0.047507304698228836, 0.017814137041568756, -0.029355106875300407, -0.020721938461065292, -0.00914685521274805, -0.03702700138092041, -0.00035887330886907876, 0.01554032601416111, 0.09361840784549713, -0.004885027185082436, -0.06969626992940903, 0.06942163407802582, 0.037106774747371674, -0.07186876982450485, 0.004690123721957207, -0.027291184291243553, 0.021349988877773285, 0.10465746372938156, 0.015400818549096584, -0.024354584515094757, 0.032575689256191254, 0.003950139041990042, 0.08484240621328354, 0.0691649466753006, 0.06575359404087067, -0.014882915653288364, -0.046410784125328064, 0.04101874306797981, -0.004657070152461529, 0.07875178009271622, -0.026855938136577606, -0.05883164703845978, -0.04814937710762024, -0.027709467336535454, -0.09450531750917435, 0.008944494649767876, 0.021495820954442024, 0.04393157735466957, -0.056054238229990005, 0.006660872139036655, -0.030360618606209755, -0.018059708178043365, 0.04132361710071564, -0.03804364427924156, 0.037537965923547745, 0.06098203361034393, -0.05958076938986778, 0.006211801897734404, -0.03772871196269989, 0.08102764189243317, 0.14073756337165833, -0.14642778038978577, 0.03235287219285965, -0.061960987746715546, 0.048154067248106, -0.06212727725505829, -4.789899676893583e-08, 0.0202272180467844, 0.05767686292529106, -0.008531808853149414, 0.027675427496433258, -0.048033207654953, 0.01932118646800518, -0.07945910096168518, 0.024194873869419098, 0.04644853249192238, 0.06892954558134079, 0.11047149449586868, 0.07858437299728394, 0.08424872905015945, 0.029783299192786217, -0.03480740264058113, -0.05810149013996124, -0.0284869484603405, 0.01727352663874626, 0.016737675294280052, 0.004047698806971312, 0.04004422575235367, 0.03884965553879738, -0.02427462488412857, -0.05644417181611061, 0.08140376955270767, -0.04860543832182884, 0.0019242899725213647, 0.09474796801805496, 0.043092384934425354, -0.04604491963982582, 0.012739705853164196, 0.03637169301509857, 0.025534706190228462, -0.013665868900716305, -0.018673604354262352, 0.013458292931318283, 0.023102957755327225, 0.06324572116136551, 0.029106341302394867, 0.0010158315999433398, 0.0051572201773524284, 0.05359832942485809, -0.029162712395191193, 0.03945532068610191, -0.09847582131624222, -0.02861184813082218, -0.08303874731063843, -0.03402744233608246, 0.06920764595270157, 0.019753390923142433, -0.08377861976623535, -0.029324602335691452, 0.0692477896809578, 0.048853226006031036, 0.014567377045750618, 0.031493719667196274, -0.01711447723209858, -0.016945786774158478, 0.007140707224607468, 0.050103165209293365, 0.06112145259976387, -0.08165576308965683, -0.11978871375322342, 0.01650512032210827]]\n"
     ]
    }
   ],
   "source": [
    "## Embedding the words to vectors\n",
    "import chromadb\n",
    "from chromadb.utils.embedding_functions import SentenceTransformerEmbeddingFunction\n",
    "\n",
    "embedding_function = SentenceTransformerEmbeddingFunction()\n",
    "print(embedding_function([character_split_texts[10]])) # An example of embedding result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c57b342-a2b5-480a-9c3b-46ab4dc9c9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Add all embedding results to Database(Long term memory) and indexed them\n",
    "\n",
    "chroma_client = chromadb.Client()\n",
    "chroma_collection = chroma_client.create_collection(\"IPCC_AR6_WGIII_FullReport.pdf\", embedding_function=embedding_function)\n",
    "\n",
    "ids = [str(i) for i in range(len(character_split_texts))]\n",
    "\n",
    "chroma_collection.add(ids=ids, documents=character_split_texts)\n",
    "chroma_collection.count() # indexed embedding chunks in memory(should be the same as number of chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "26d876df-cd6b-469e-9792-e67eec941a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "agricultural markets and food security (Havlík et al. 2014; Hasegawa \n",
      "et al. 2018; Doelman et al. 2019; Fujimori et al. 2019). Mitigation \n",
      "policies aimed at achieving  1.5°C–2°C, if not managed properly, Box 3.6 (continued)\n",
      "20\n",
      "051015tCO2-eq yr–1\n",
      "2030 2050\n",
      "YearC1: limit warming \n",
      "to 1.5°C (>50%) \n",
      "with no or limited \n",
      "overshoot\n",
      "C2: return \n",
      "warming to 1.5°C \n",
      "(>50%) after a \n",
      "high overshoot\n",
      "C3: limit warming \n",
      "to 2°C (>67%)\n",
      "C4: limit warming \n",
      "to 2°C (>50%)\n",
      "C5: limit warming \n",
      "to 2.5°C (>50%)\n",
      "C6: limit warming \n",
      "to 3°C (>50%)\n",
      "C7: limit warming \n",
      "to 4°C (>50%)\n",
      "C8: exceed \n",
      "warming of 4°C \n",
      "(≥50%)\n",
      "Box 3.6, Figure 1 | Difference in per-capita emissions of Kyoto gases between the highest emitting and the lowest emitting of the 10 regions, \n",
      "in 2030 and 2050, by temperature category of pathways.\n",
      "Through avoiding impacts of climate change, which fall more heavily on low-income countries, communities and households, and \n",
      "exacerbate poverty, mitigation reduces inequalities and poverty (Section 3.6.4.2).\n",
      "\n",
      "\n",
      "of >50%.97 129\n",
      "C7: Limit warming to 4°C (>50%)Limit peak warming to 4°C throughout the 21st century with a likelihood \n",
      "of >50%.164 230\n",
      "C8: Exceed warming of 4°C (≥50%) Exceed warming of 4°C during the 21st century with a likelihood of ≥50%. 29 40\n",
      "No climate assessment Scenario time horizon <2100; insufficient emissions species reported. 484 692\n",
      "Total: 1686 2266\n",
      "\n",
      "\n",
      "Limited overshoot refers to exceeding 1.5°C by up to about 0.1°C and for \n",
      "up to several decades.97 160\n",
      "C2: Return warming to 1.5°C (>50%) \n",
      "after a high overshootExceed warming of 1.5°C during the 21st century with a likelihood of >67%,  \n",
      "and limit warming to 1.5°C in 2100 with a likelihood of >50%.\n",
      "High overshoot refers to temporarily exceeding 1.5°C global warming \n",
      "by 0.1°C–0.3°C for up to several decades.133 170\n",
      "C3: Limit warming to 2°C (>67%)Limit peak warming to 2°C throughout the 21st century with a likelihood \n",
      "of >67%.311 374\n",
      "C4: Limit warming to 2oC (>50%)Limit peak warming to 2°C throughout the 21st century with a likelihood \n",
      "of >50%.159 213\n",
      "C5: Limit warming to 2.5°C (>50%)Limit peak warming to 2.5°C throughout the 21st century with a likelihood \n",
      "of >50%.212 258\n",
      "C6: Limit warming to3°C (>50%)Limit peak warming to 3°C throughout the 21st century with a likelihood \n",
      "of >50%.97 129\n",
      "\n",
      "\n",
      "with 1.5°C by a large margin and are near the upper end of the range \n",
      "of modelled pathways that limit warming to 2°C (>67%) or below. \n",
      "In all chapters of this report there is evidence of progress towards \n",
      "deeper mitigation, but there remain many obstacles to be overcome. \n",
      "Table TS.1 summarises some of the key signs of progress in emission \n",
      "trends, sectors, policies and investment, as well as the challenges \n",
      "that persist. Table TS.1 (continued):\n",
      "\n",
      "\n",
      "• Category C1 comprises modelled scenarios that limit warming to 1.5°C in 2100 with a likelihood of greater than 50%, \n",
      "and reach or exceed warming of 1.5°C during the 21st century with a likelihood of 67% or less. In this report, these scenarios \n",
      "are referred to as scenarios that limit warming to 1.5°C (>50%) with no or limited overshoot. Limited overshoot refers to \n",
      "exceeding 1.5°C global warming by up to about 0.1°C and for up to several decades.48 \n",
      "• Category C2 comprises modelled scenarios that limit warming to 1.5°C in 2100 with a likelihood of greater than 50%, \n",
      "and exceed warming of 1.5°C during the 21st century with a likelihood of greater than 67%. In this report, these scenarios \n",
      "are also referred to as scenarios that return warming to 1.5°C (>50%) after a high overshoot. High overshoot refers to \n",
      "temporarily exceeding 1.5°C global warming by 0.1°C–0.3°C for up to several decades.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Embedding our question and do the semantic similarity search\n",
    "# print out the top 5 similar chunks\n",
    "\n",
    "# question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "query = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "for document in retrieved_documents:\n",
    "    print(document)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc12f09-7699-4238-9651-dcc0cc837237",
   "metadata": {},
   "source": [
    "## ChatGPT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ad02d7",
   "metadata": {},
   "source": [
    "Our external knowledge is ready. Next is about ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f719b9-67ff-4637-8996-133cc1c08724",
   "metadata": {},
   "source": [
    "### How to run ChatGPT-3.5 + pdf (chatClimate)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3eeb61e-9431-4f4c-bf2c-517a90d3878e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "import langchain\n",
    "langchain.debug = True\n",
    "\n",
    "def get_chatclimate(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a Q&A bot, an intelligent system that answers user questions ONLY based on the information provided by the user. If you use user information, please indicate the Page and Reference, which are provided below each piece of information. If the information cannot be found in the information provided by the user, please say ’I don’t know’\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94368852-44d8-4c2a-8208-2c832a972545",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the information provided, it is still possible to limit warming to 1.5°C. Modelled scenarios in Category C1 aim to limit warming to 1.5°C with a likelihood greater than 50% and reach or exceed warming of 1.5°C during the 21st century with a likelihood of 67% or less. These scenarios are referred to as scenarios that limit warming to 1.5°C (>50%) with no or limited overshoot. Limited overshoot refers to exceeding 1.5°C global warming by up to about 0.1°C and for up to several decades (Box 3.6). Therefore, these modelled scenarios show that it is possible to limit warming to 1.5°C, but there are potential challenges and obstacles to overcome (Table TS.1).\n"
     ]
    }
   ],
   "source": [
    "# question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "query = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "# for document in retrieved_documents:\n",
    "#     print(document)\n",
    "#     print('\\n')\n",
    "\n",
    "output = get_chatclimate(query=query, retrieved_documents=retrieved_documents)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e63769a",
   "metadata": {},
   "source": [
    "### How to run ChatGPT-3.5 hybrid chatClimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "855d8062-4a40-45e9-802a-c4242fe19ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "\n",
    "def get_hybrid(query, retrieved_documents, model=\"gpt-3.5-turbo\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a Q&A bot, an intelligent system that answers user questions based on the information provided by the user above the question and your in-house knowledge. There are 5 pieces of extra information above the user question. please indicate the Page and Reference, which are provided below each piece of information. Additionally, let us know which part of your answer is from the IPCC information and which part is based on your in-house knowledge by writing either (IPCC AR6) or (Inhouse knowledge). If the information cannot be found in the information provided by the user or your in-house knowledge, please say ’I don’t know’.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cfd1ece9-704e-49a0-b509-1ec9246a3b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the information provided, there are modelled scenarios (Category C1) that limit warming to 1.5°C in 2100 with a likelihood of greater than 50%. These scenarios are referred to as scenarios that limit warming to 1.5°C (>50%) with no or limited overshoot. Limited overshoot refers to exceeding 1.5°C global warming by up to about 0.1°C and for up to several decades (IPCC AR6). However, it is important to note that there are still many obstacles to overcome in achieving these scenarios (IPCC AR6).\n"
     ]
    }
   ],
   "source": [
    "# question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "query = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "\n",
    "# query = joint_query\n",
    "\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "# for document in retrieved_documents:\n",
    "#     print(document)\n",
    "#     print('\\n')\n",
    "\n",
    "output = get_hybrid(query=query, retrieved_documents=retrieved_documents)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb60995-d509-441d-9038-b8b1e8b9e67a",
   "metadata": {},
   "source": [
    "### How does ChatGPT-4 answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a90a2a0-296f-4a12-899d-96ac057cb4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ChatGPT(query, model=\"gpt-4\"):\n",
    "    # information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}.\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "683a21cb-4ae8-48b0-927e-8f1ca9adcb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: Studies suggest that it is technically possible to limit global warming to 1.5°C, but it would require substantial and immediate reductions in greenhouse gas emissions. This includes rapid, far-reaching changes in all aspects of society, like energy, land use, cities, and industry. However, achieving this goal remains highly challenging due to a variety of political, economic, and technological obstacles.\n"
     ]
    }
   ],
   "source": [
    "query = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "output = get_ChatGPT(query=query)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc77296-8b61-47b6-8328-6cc2ca4f43ed",
   "metadata": {},
   "source": [
    "### How to run ChatGPT-4+pdf(chatClimate)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cca7ee69-a953-4e62-b71a-eaf9225ac973",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def get_chatclimate(query, retrieved_documents, model=\"gpt-4\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a Q&A bot, an intelligent system that answers user questions ONLY based on the information provided by the user. If you use user information, please indicate the Page and Reference, which are provided below each piece of information. If the information cannot be found in the information provided by the user, please say ’I don’t know’\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7b6cb19-dbc5-4cbe-90d1-eaa5c859faff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided information, it appears that it is theoretically possible to limit warming to 1.5°C. There are modelled scenarios, such as Category C1 and C2, that aim to limit warming to this level.  Category C1 is about limiting warming to 1.5°C in 2100 with a likelihood of greater than 50%, with no or limited overshoot. Limited overshoot refers to momentarily exceeding 1.5°C global warming by up to about 0.1°C for up to several decades. Category C2 represents scenarios that return warming to 1.5°C in 2100 with a likelihood of greater than 50% after a high overshoot. High overshoot refers to exceeding 1.5°C global warming by 0.1°C–0.3°C for up to several decades. However, the provided information mentions that there are many obstacles yet to overcome to achieve these temperature targets (Page 1, third to last paragraph).\n"
     ]
    }
   ],
   "source": [
    "# question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "query = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "# for document in retrieved_documents:\n",
    "#     print(document)\n",
    "#     print('\\n')\n",
    "\n",
    "output = get_chatclimate(query=query, retrieved_documents=retrieved_documents)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70db697-9cd0-42c2-bbb6-379f24b74eac",
   "metadata": {},
   "source": [
    "### How to run ChatGPT-4 hybird chatClimate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2520a0bf-285c-4337-aac0-bb4c6ac1df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "def get_hybrid(query, retrieved_documents, model=\"gpt-4\"):\n",
    "    information = \"\\n\\n\".join(retrieved_documents)\n",
    "\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a Q&A bot, an intelligent system that answers user questions based on the information provided by the user above the question and your in-house knowledge. There are 5 pieces of extra information above the user question. please indicate the Page and Reference, which are provided below each piece of information. Additionally, let us know which part of your answer is from the IPCC information and which part is based on your in-house knowledge by writing either (IPCC AR6) or (Inhouse knowledge). If the information cannot be found in the information provided by the user or your in-house knowledge, please say ’I don’t know’.\"\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"Question: {query}. \\n Information: {information}\"}\n",
    "    ]\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "    )\n",
    "    content = response.choices[0].message.content\n",
    "    return content\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bd8d3a6a-a047-4e2a-9905-050e0f4a65de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, it is still theoretically possible to limit warming to 1.5°C. According to the information provided, there are modelled scenarios that predict limiting warming to 1.5°C in 2100 with a likelihood of greater than 50% and scenarios that return warming to 1.5°C (>50%) after a high overshoot. The high overshoot refers to temporarily exceeding 1.5°C global warming by 0.1°C–0.3°C for up to several decades (Page: Box 3.6, Figure 1, Reference 48). My answer is based on the information from IPCC AR6. However, it is important to add (Inhouse knowledge) that reaching these goals would require significant efforts to reduce greenhouse gas emissions immediately as well as implement various mitigation strategies.\n"
     ]
    }
   ],
   "source": [
    "# question = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "query = \"Is it still possible to limit warming to 1.5°C?\" #Q1\n",
    "\n",
    "results = chroma_collection.query(query_texts=[query], n_results=5)\n",
    "retrieved_documents = results['documents'][0]\n",
    "\n",
    "# for document in retrieved_documents:\n",
    "#     print(document)\n",
    "#     print('\\n')\n",
    "\n",
    "output = get_hybrid(query=query, retrieved_documents=retrieved_documents)\n",
    "\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
