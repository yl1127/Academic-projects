{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2\n",
    "\n",
    "**AMS560 - Fall 2024**  \n",
    "**Professor:** Zhenhua Liu  \n",
    "**Teaching Assistants:** Yunlong Pan & Xander Barron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "\n",
    "### Section 1: Download and run\n",
    "\n",
    "1. (10 points)Do you sucessfully set up a Cloudlab with GPU? (Yes or No and a screenshot)\n",
    "2. (10 points)Do you sucessfully print the model list? (Yes or No and a screenshot)\n",
    "3. (10 points)Do you sucessfully download the **Meta-Llama3.1-8B-Instruct** model? Where you download it? (Yes or No and a screenshot)\n",
    "4. (10 points)Do you sucessfully install GPU driver? Show your GPU usage. (Yes or No and a screenshot)\n",
    "5. (10 points)Do you sucessfully run the **example_chat_completion.py**? (Yes or No and three screenshots)\n",
    "\n",
    "### Section 2: Explore\n",
    "\n",
    "6. (10 points)How many files in **/Meta-Llama3.1-8B-Instruct**? What are they?\n",
    "7. (10 points)How many heads of each multi-head attention block?\n",
    "8. (10 points)What's the tokenizer encode output of \"hello world!\"?\n",
    "9. (10 points)How many transformer layers of this model?\n",
    "10. (10 points)What's the shape of \"layers.21.attention.wo.weight\"?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. [Set up Cloudlab](#1-set-up-cloudlab)\n",
    "2. [Request Access to Llama Models](#2-request-access-to-llama-models)\n",
    "3. [Download the Llama Model](#3-download-the-llama-model)\n",
    "4. [Install GPU Driver](#4-install-gpu-driver)\n",
    "5. [Run Llama Model](#5-run-llama-model)\n",
    "6. [Explore](#6-explore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set up Cloudlab\n",
    "\n",
    "- Profile: OpenStack\n",
    "- Number of computer nodes: 0\n",
    "- Hardware type(a GPU with at least 20GB RAM is required)\n",
    "- Hardware type Example: Cloudlab Wisconsin->d7525 (For Q1, take a screenshot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![GPUlist](Screenshot_GPUlist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![cloudlab](Screenshot_cloudlab.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Request Access to Llama Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Llama: Download models https://llama.meta.com/\n",
    "- You will get an email when you finish your request."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Llama](Screenshot_Llama.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![request](Screenshot_request.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![email](Screenshot_email.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Download the Llama Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Follow the download instruction: https://github.com/meta-llama/llama-models\n",
    "- Remember your download path. Example '/users/ylpan/.llama/checkpoints/Meta-Llama3.1-8B-Instruct'\n",
    "\n",
    "```bash\n",
    "pip install llama-toolchain\n",
    "\n",
    "llama model list --show-all\n",
    "# if llama: command not found, use the full path. Example:'/users/ylpan/.local/bin/llama model list --show-all'\n",
    "# For Q2, take a screenshot\n",
    "\n",
    "llama download --source meta --model-id Meta-Llama3.1-8B-Instruct\n",
    "# We choose Llama3.1-8b-instruct\n",
    "# For Q3, take a screenshot\n",
    "\n",
    "# if llama: command not found, use the full path. Example:'/users/ylpan/.local/bin/llama download --source meta --model-id Meta-Llama3.1-8B-Instruct'\n",
    "\n",
    "# Provide the signed URL you received via email\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Download](Screenshot_download.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![pip](Screenshot_pip.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![modellist](Screenshot_modellist.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![done](Screenshot_done.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Install GPU Driver\n",
    "\n",
    "- Check your Cloudlab hardware again. At least d7525(A30 with RAM 24 GB) is recommended.\n",
    "- NVIDIA drivers installation https://ubuntu.com/server/docs/nvidia-drivers-installation\n",
    "- Your system need some time to reboot.\n",
    "\n",
    "```bash\n",
    "sudo apt install ubuntu-drivers-common\n",
    "\n",
    "sudo ubuntu-drivers list\n",
    "# check if ubuntu-drivers install successfully\n",
    "\n",
    "sudo ubuntu-drivers install\n",
    "\n",
    "sudo reboot\n",
    "# reboot the system after ubuntu-drivers install\n",
    "# you will loss connection and this step need some time\n",
    "\n",
    "# After reboot, reconnect with your system and check if NVIDIA drivers install successfully or not\n",
    "nvidia-smi\n",
    "# if error: NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running. Try to repeat the above steps\n",
    "# For Q4, take a screenshot\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![hardware](Screenshot_hardware.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![usage](Screenshot_usage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Llama Model\n",
    "\n",
    "- Obtain the scripts from Llama github\n",
    "- Follow the **Running the models** section https://github.com/meta-llama/llama-models\n",
    "- Prepare python environment\n",
    "- Run an example\n",
    "- Three conversitions from example scripts will show up if you run the example successfully.\n",
    "\n",
    "```bash\n",
    "git clone https://github.com/meta-llama/llama-models.git\n",
    "\n",
    "cd llama-models/\n",
    "\n",
    "pip install -r requirements.txt \n",
    "\n",
    "pip install torch fairscale fire blobfile\n",
    "\n",
    "nano run_example.sh\n",
    "```\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "\n",
    "CHECKPOINT_DIR=~/.llama/checkpoints/Meta-Llama3.1-8B-Instruct\n",
    "PYTHONPATH=$(git rev-parse --show-toplevel) torchrun models/scripts/example_chat_completion.py $CHECKPOINT_DIR\n",
    "```\n",
    "\n",
    "```bash\n",
    "chmod +x ./run_example.sh\n",
    "\n",
    "./run_example.sh\n",
    "\n",
    "# For Q5, take three screenshots.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![running](Screenshot_running.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example1](Screenshot_example1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example2](Screenshot_example2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![example3](Screenshot_example3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Install the python packages\n",
    "- Explore the model you download\n",
    "- Read and run the python file first\n",
    "- Modify the python file to answer Q6-10\n",
    "\n",
    "```bash\n",
    "pip install sentencepiece tiktoken torch blobfile matplotlib\n",
    "# Install the python packages\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```bash\n",
    "nano AMS560_HW2.py\n",
    "# create a python file\n",
    "python AMS560_HW2.py\n",
    "# run it\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "========================\n",
    "\n",
    "This is your example python file. Read and run this first. Then finish your Q6-10 by modifying this.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import tiktoken\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "import torch\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "path = \"...\"\n",
    "# modify the path first\n",
    "# For Q6, explore this path\n",
    "# For Q7, explore 'params.json' file\n",
    "\n",
    "tokenizer_path = path + \"/tokenizer.model\"\n",
    "special_tokens = [\n",
    "            \"<|begin_of_text|>\",\n",
    "            \"<|end_of_text|>\",\n",
    "            \"<|reserved_special_token_0|>\",\n",
    "            \"<|reserved_special_token_1|>\",\n",
    "            \"<|reserved_special_token_2|>\",\n",
    "            \"<|reserved_special_token_3|>\",\n",
    "            \"<|start_header_id|>\",\n",
    "            \"<|end_header_id|>\",\n",
    "            \"<|reserved_special_token_4|>\",\n",
    "            \"<|eot_id|>\",  # end of turn\n",
    "        ] + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]\n",
    "mergeable_ranks = load_tiktoken_bpe(tokenizer_path)\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=Path(tokenizer_path).name,\n",
    "    pat_str=r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\",\n",
    "    mergeable_ranks=mergeable_ranks,\n",
    "    special_tokens={token: len(mergeable_ranks) + i for i, token in enumerate(special_tokens)},\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(tokenizer.encode(\"hello world!\")))\n",
    "# For Q8, print the tokenizer.encode() output\n",
    "\n",
    "model = torch.load(path+\"/consolidated.00.pth\", weights_only=True)\n",
    "print('This is the first 20 matrices weights:')\n",
    "print(json.dumps(list(model.keys())[:20], indent=4))\n",
    "# This show the first 20 matrices weights. \n",
    "# For Q9, show last 5 or them all.\n",
    "\n",
    "print('This is the shape for feed forward W_2 of layer 0, ')\n",
    "print(model[\"layers.0.feed_forward.w2.weight\"].shape)\n",
    "# For Q10, print the shape for attention W_o of layer 21.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![explore](Screenshot_explore1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
