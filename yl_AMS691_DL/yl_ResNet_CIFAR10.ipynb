{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Deep Learning Model for CIFAR-10 Image Classification\n",
        "\n",
        "Name: Yunlong Pan\n",
        "\n",
        "ID#: 113061415\n",
        "\n",
        "- Developed a deep learning model for image classification on the CIFAR-10 dataset, achieving an accuracy of 91.4%.\n",
        "- Implemented a ResNet20 architecture in PyTorch to improve model performance.\n",
        "\n",
        "Tools: PyTorch, Python, Convolutional Neural Networks (CNN), ResNet"
      ],
      "metadata": {
        "id": "Rz-d8M1PXw5N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "uSq1ptWyw3fO"
      },
      "outputs": [],
      "source": [
        "# For tips on running notebooks in Google Colab, see\n",
        "# https://pytorch.org/tutorials/beginner/colab\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTE3Oq5Gw3fQ"
      },
      "source": [
        "\n",
        "# Quickstart\n",
        "This section runs through the API for common tasks in machine learning. Refer to the links in each section to dive deeper.\n",
        "\n",
        "## Working with data\n",
        "PyTorch has two [primitives to work with data](https://pytorch.org/docs/stable/data.html):\n",
        "``torch.utils.data.DataLoader`` and ``torch.utils.data.Dataset``.\n",
        "``Dataset`` stores the samples and their corresponding labels, and ``DataLoader`` wraps an iterable around\n",
        "the ``Dataset``.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VZdLvK27w3fS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.init as init\n",
        "from torch.autograd import Variable\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kb5B2JZ0w3fS"
      },
      "source": [
        "PyTorch offers domain-specific libraries such as [TorchText](https://pytorch.org/text/stable/index.html),\n",
        "[TorchVision](https://pytorch.org/vision/stable/index.html), and [TorchAudio](https://pytorch.org/audio/stable/index.html),\n",
        "all of which include datasets. For this tutorial, we  will be using a TorchVision dataset.\n",
        "\n",
        "The ``torchvision.datasets`` module contains ``Dataset`` objects for many real-world vision data like\n",
        "CIFAR, COCO ([full list here](https://pytorch.org/vision/stable/datasets.html)). In this tutorial, we\n",
        "use the CIFAR-10 dataset. Every TorchVision ``Dataset`` includes two arguments: ``transform`` and\n",
        "``target_transform`` to modify the samples and labels respectively.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "C5J3Jn19w3fS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da868008-493b-41b5-f82c-f3530df9447f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 44404325.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "# Download training data from open datasets.\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WJfzH9pw3fT"
      },
      "source": [
        "We pass the ``Dataset`` as an argument to ``DataLoader``. This wraps an iterable over our dataset, and supports\n",
        "automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 64, i.e. each element\n",
        "in the dataloader iterable will return a batch of 64 features and labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vUjkMp3Lw3fT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4caf5650-e68f-460a-a254-307c6dbdd01a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X [N, C, H, W]: torch.Size([64, 3, 32, 32])\n",
            "Shape of y: torch.Size([64]) torch.int64\n"
          ]
        }
      ],
      "source": [
        "batch_size = 64\n",
        "\n",
        "# Create data loaders.\n",
        "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
        "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
        "\n",
        "for X, y in test_dataloader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f-9Cd7Kw3fT"
      },
      "source": [
        "Read more about [loading data in PyTorch](data_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye7p97oxw3fT"
      },
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHvThyFiw3fT"
      },
      "source": [
        "## Creating Models\n",
        "To define a neural network in PyTorch, we create a class that inherits\n",
        "from [nn.Module](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We define the layers of the network\n",
        "in the ``__init__`` function and specify how data will pass through the network in the ``forward`` function. To accelerate\n",
        "operations in the neural network, we move it to the GPU or MPS if available.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "9EBrOqeyw3fT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0c141f2-46b1-454e-e010-1c3bd5af535d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "DataParallel(\n",
            "  (module): ResNet(\n",
            "    (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "    (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (layer1): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "    )\n",
            "    (layer2): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): LambdaLayer()\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "    )\n",
            "    (layer3): Sequential(\n",
            "      (0): BasicBlock(\n",
            "        (conv1): Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): LambdaLayer()\n",
            "      )\n",
            "      (1): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "      (2): BasicBlock(\n",
            "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "        (shortcut): Sequential()\n",
            "      )\n",
            "    )\n",
            "    (linear): Linear(in_features=64, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Get cpu, gpu or mps device for training.\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "# Define model\n",
        "__all__ = ['ResNet', 'resnet20', 'resnet32', 'resnet44', 'resnet56', 'resnet110', 'resnet1202']\n",
        "\n",
        "def _weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    #print(classname)\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        init.kaiming_normal_(m.weight)\n",
        "\n",
        "class LambdaLayer(nn.Module):\n",
        "    def __init__(self, lambd):\n",
        "        super(LambdaLayer, self).__init__()\n",
        "        self.lambd = lambd\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.lambd(x)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1, option='A'):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != planes:\n",
        "            if option == 'A':\n",
        "                \"\"\"\n",
        "                For CIFAR10 ResNet paper uses option A.\n",
        "                \"\"\"\n",
        "                self.shortcut = LambdaLayer(lambda x:\n",
        "                                            F.pad(x[:, :, ::2, ::2], (0, 0, 0, 0, planes//4, planes//4), \"constant\", 0))\n",
        "            elif option == 'B':\n",
        "                self.shortcut = nn.Sequential(\n",
        "                     nn.Conv2d(in_planes, self.expansion * planes, kernel_size=1, stride=stride, bias=False),\n",
        "                     nn.BatchNorm2d(self.expansion * planes)\n",
        "                )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 16\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
        "        self.linear = nn.Linear(64, num_classes)\n",
        "\n",
        "        self.apply(_weights_init)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = F.avg_pool2d(out, out.size()[3])\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "def resnet20():\n",
        "    return ResNet(BasicBlock, [3, 3, 3])\n",
        "\n",
        "\n",
        "def resnet32():\n",
        "    return ResNet(BasicBlock, [5, 5, 5])\n",
        "\n",
        "\n",
        "def resnet44():\n",
        "    return ResNet(BasicBlock, [7, 7, 7])\n",
        "\n",
        "\n",
        "def resnet56():\n",
        "    return ResNet(BasicBlock, [9, 9, 9])\n",
        "\n",
        "\n",
        "def resnet110():\n",
        "    return ResNet(BasicBlock, [18, 18, 18])\n",
        "\n",
        "\n",
        "def resnet1202():\n",
        "    return ResNet(BasicBlock, [200, 200, 200])\n",
        "\n",
        "\n",
        "def test(net):\n",
        "    import numpy as np\n",
        "    total_params = 0\n",
        "\n",
        "    for x in filter(lambda p: p.requires_grad, net.parameters()):\n",
        "        total_params += np.prod(x.data.numpy().shape)\n",
        "    # print(\"Total number of params\", total_params)\n",
        "    # print(\"Total layers\", len(list(filter(lambda p: p.requires_grad and len(p.data.size())>1, net.parameters()))))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for net_name in __all__:\n",
        "        if net_name.startswith('resnet'):\n",
        "            # print(net_name)\n",
        "            test(globals()[net_name]())\n",
        "            # print()\n",
        "\n",
        "model = resnet20().to(device)\n",
        "model = torch.nn.DataParallel(model)\n",
        "model.load_state_dict(torch.load('resnet20-12fca82f.th')['state_dict'])\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHDFL1bzw3fU"
      },
      "source": [
        "Read more about [building neural networks in PyTorch](buildmodel_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gtO4e1FWw3fU"
      },
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwXr1-Nzw3fU"
      },
      "source": [
        "## Optimizing the Model Parameters\n",
        "To train a model, we need a [loss function](https://pytorch.org/docs/stable/nn.html#loss-functions)\n",
        "and an [optimizer](https://pytorch.org/docs/stable/optim.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BsJBW4ipw3fU"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TwzcIxR7w3fU"
      },
      "source": [
        "In a single training loop, the model makes predictions on the training dataset (fed to it in batches), and\n",
        "backpropagates the prediction error to adjust the model's parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "PJlDkSBNw3fV"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # Compute prediction error\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), (batch + 1) * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2d-iVYqw3fV"
      },
      "source": [
        "We also check the model's performance against the test dataset to ensure it is learning.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cgHBzZ0ww3fV"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkqojG4Ww3fV"
      },
      "source": [
        "The training process is conducted over several iterations (*epochs*). During each epoch, the model learns\n",
        "parameters to make better predictions. We print the model's accuracy and loss at each epoch; we'd like to see the\n",
        "accuracy increase and the loss decrease with every epoch.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2osZyrPtw3fV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c9a5c6-87c4-4752-8ed6-5827a3c27a2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 0.141099  [   64/50000]\n",
            "loss: 0.181717  [ 6464/50000]\n",
            "loss: 0.057134  [12864/50000]\n",
            "loss: 0.072456  [19264/50000]\n",
            "loss: 0.009936  [25664/50000]\n",
            "loss: 0.055640  [32064/50000]\n",
            "loss: 0.066944  [38464/50000]\n",
            "loss: 0.124861  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 90.2%, Avg loss: 0.409653 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.060003  [   64/50000]\n",
            "loss: 0.038974  [ 6464/50000]\n",
            "loss: 0.020278  [12864/50000]\n",
            "loss: 0.037820  [19264/50000]\n",
            "loss: 0.007986  [25664/50000]\n",
            "loss: 0.024010  [32064/50000]\n",
            "loss: 0.052780  [38464/50000]\n",
            "loss: 0.079477  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 90.9%, Avg loss: 0.381950 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.036861  [   64/50000]\n",
            "loss: 0.014995  [ 6464/50000]\n",
            "loss: 0.009443  [12864/50000]\n",
            "loss: 0.020816  [19264/50000]\n",
            "loss: 0.007399  [25664/50000]\n",
            "loss: 0.017498  [32064/50000]\n",
            "loss: 0.044340  [38464/50000]\n",
            "loss: 0.054466  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 91.1%, Avg loss: 0.371516 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.028978  [   64/50000]\n",
            "loss: 0.011049  [ 6464/50000]\n",
            "loss: 0.007341  [12864/50000]\n",
            "loss: 0.016065  [19264/50000]\n",
            "loss: 0.007278  [25664/50000]\n",
            "loss: 0.014641  [32064/50000]\n",
            "loss: 0.037574  [38464/50000]\n",
            "loss: 0.041973  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 91.2%, Avg loss: 0.366129 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.023879  [   64/50000]\n",
            "loss: 0.009588  [ 6464/50000]\n",
            "loss: 0.006115  [12864/50000]\n",
            "loss: 0.014101  [19264/50000]\n",
            "loss: 0.007039  [25664/50000]\n",
            "loss: 0.012788  [32064/50000]\n",
            "loss: 0.033040  [38464/50000]\n",
            "loss: 0.035524  [44864/50000]\n",
            "Test Error: \n",
            " Accuracy: 91.4%, Avg loss: 0.362680 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train(train_dataloader, model, loss_fn, optimizer)\n",
        "    test(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW4DP3JCw3fV"
      },
      "source": [
        "Read more about [Training your model](optimization_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGZ7BMQkw3fV"
      },
      "source": [
        "--------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wN3Y_sGtw3fW"
      },
      "source": [
        "## Saving Models\n",
        "A common way to save a model is to serialize the internal state dictionary (containing the model parameters).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "t4yBI4xsw3fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b25148c6-5317-4d2f-d6d2-10b857ee14f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved PyTorch Model State to model.pth\n"
          ]
        }
      ],
      "source": [
        "torch.save(model.state_dict(), \"model.pth\")\n",
        "print(\"Saved PyTorch Model State to model.pth\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXb3g7YHw3fW"
      },
      "source": [
        "## Loading Models\n",
        "\n",
        "The process for loading a model includes re-creating the model structure and loading\n",
        "the state dictionary into it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "zc29HaMLw3fW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dfae4dce-0233-4c84-8cac-1c80f1205ae6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "# model = nn.DataParallel(resnet20())\n",
        "model = resnet20().to(device)\n",
        "model = torch.nn.DataParallel(model)\n",
        "model.load_state_dict(torch.load(\"model.pth\"))\n",
        "# model = torch.nn.DataParallel(model)\n",
        "# model.load_state_dict(torch.load('resnet20-12fca82f.th')['state_dict'])\n",
        "# model = torch.load('resnet20-12fca82f.th')['state_dict']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MFxxrhC4w3fW"
      },
      "source": [
        "This model can now be used to make predictions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "jjk-m8F6w3fW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "outputId": "c9ca1d99-8224-4e2a-f459-e1f9930cc14a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 5.0208, -1.5331, -3.5464, 12.1908, -8.2495,  7.9666, -8.7564,  2.8027,\n",
            "        -3.3110, -2.6994], device='cuda:0')\n",
            "Predicted: \"cat\", Actual: \"dog\"\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "classes = [\n",
        "    \"airplane\",\n",
        "    \"automobile\",\n",
        "    \"bird\",\n",
        "    \"cat\",\n",
        "    \"deer\",\n",
        "    \"dog\",\n",
        "    \"frog\",\n",
        "    \"horse\",\n",
        "    \"ship\",\n",
        "    \"truck\",\n",
        "]\n",
        "\n",
        "model.eval()\n",
        "num = np.random.randint(0, 10000)\n",
        "x, y = test_data[num][0], test_data[num][1]\n",
        "plt.imshow(np.transpose(x, [1, 2, 0]))\n",
        "with torch.no_grad():\n",
        "    x = x.unsqueeze(0).to(device)\n",
        "    pred = model(x)\n",
        "    print(pred[0])\n",
        "    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n",
        "    print(f'Predicted: \"{predicted}\", Actual: \"{actual}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_idbSjngw3fW"
      },
      "source": [
        "Read more about [Saving & Loading your model](saveloadrun_tutorial.html).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem\n",
        "\n",
        "1. using GPU or CPU\n",
        "2. be careful of the input shape and type"
      ],
      "metadata": {
        "id": "xgilkEAMb12v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference\n",
        "\n",
        "[1] Deep Residual Learning for Image Recognition (https://arxiv.org/abs/1512.03385)\n",
        "\n",
        "[2] Identity Mappings in Deep Residual Networks (https://arxiv.org/abs/1603.05027)\n",
        "\n",
        "[3] Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift (https://arxiv.org/abs/1502.03167)\n",
        "\n",
        "[4] Proper ResNet Implementation for CIFAR10/CIFAR100 in PyTorch(https://github.com/akamaster/pytorch_resnet_cifar10)\n"
      ],
      "metadata": {
        "id": "Q7cOqZlLW27T"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}